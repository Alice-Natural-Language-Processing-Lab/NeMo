{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Megatron-LM_for_Downstream_Tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X87q4jmW4Rmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BRANCH = 'megatron_docs'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djqHSONJ20X8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c54a988e-1a77-4171-953f-0813bd95328b"
      },
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell\n",
        "\n",
        "# install NeMo\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[nlp]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nemo_toolkit[nlp]\n",
            "  Cloning https://github.com/NVIDIA/NeMo.git (to revision megatron_docs) to /tmp/pip-install-uqmwosba/nemo-toolkit\n",
            "  Running command git clone -q https://github.com/NVIDIA/NeMo.git /tmp/pip-install-uqmwosba/nemo-toolkit\n",
            "  Running command git checkout -b megatron_docs --track origin/megatron_docs\n",
            "  Switched to a new branch 'megatron_docs'\n",
            "  Branch 'megatron_docs' set up to track remote branch 'megatron_docs' from 'origin'.\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (1.18.5)\n",
            "Collecting onnx>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/ee/bc7bc88fc8449266add978627e90c363069211584b937fd867b0ccc59f09/onnx-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 2.6MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/af/2f10c8ee22d7a05fe8c9be58ad5c55b71ab4dd895b44f0156bfd5535a708/pytorch_lightning-0.9.0-py3-none-any.whl (408kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (2.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (1.6.0+cu101)\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (1.12.1)\n",
            "Collecting ruamel.yaml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/51/a5592f3fc7f0f8a939c556ba27e00fc9541c09ff9a1a5ea42463650481c3/ruamel.yaml-0.16.11-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (0.22.2.post1)\n",
            "Collecting omegaconf==2.0.1rc12\n",
            "  Downloading https://files.pythonhosted.org/packages/93/44/ec491bba178fea12f87778eca6661e63bfa4baec1bf10461937234f11271/omegaconf-2.0.1rc12-py3-none-any.whl\n",
            "Collecting hydra-core==1.0.0rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/af/90d053fd14f52a7b86eb8623b45b844e67676f20db26e5e1b666f77d8faf/hydra_core-1.0.0rc4-py3-none-any.whl (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 45.8MB/s \n",
            "\u001b[?25hCollecting transformers>=2.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (1.14.48)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (2.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (3.2.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (0.3.1)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 40.0MB/s \n",
            "\u001b[?25hCollecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (4.41.1)\n",
            "Collecting rapidfuzz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/00/d1d68b3600395557ece71ef4a119c65a09ea8cd1f27490953d99b0253885/rapidfuzz-0.11.1-cp36-cp36m-manylinux2010_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 25.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (3.6.4)\n",
            "Collecting megatron-lm>=1.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/a5/29e0a4edd782af828c6a22b4206b606e56816c3d0c5734a7cc593961ecf5/megatron_lm-1.1.4-py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (2.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (3.7.4.3)\n",
            "Collecting tensorboard==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/f5/d75a6f7935e4a4870d85770bc9976b12e7024fbceb83a1a6bc50e6deb7c4/tensorboard-2.2.0-py3-none-any.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 32.9MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 46.7MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 35.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (20.4)\n",
            "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/77/4bcd63f362bcb6c8f4f06253c11f9772f64189bf08cf3f40c5ccbda9e561/ruamel.yaml.clib-0.2.0-cp36-cp36m-manylinux1_x86_64.whl (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nemo_toolkit[nlp]) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nemo_toolkit[nlp]) (1.4.1)\n",
            "Requirement already satisfied: dataclasses; python_version == \"3.6\" in /usr/local/lib/python3.6/dist-packages (from omegaconf==2.0.1rc12->nemo_toolkit[nlp]) (0.7)\n",
            "Collecting importlib-resources; python_version < \"3.9\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/03/0f9595c0c2ef12590877f3c47e5f579759ce5caf817f8256d5dcbd8a1177/importlib_resources-3.0.0-py2.py3-none-any.whl\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->nemo_toolkit[nlp]) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->nemo_toolkit[nlp]) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->nemo_toolkit[nlp]) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->nemo_toolkit[nlp]) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->nemo_toolkit[nlp]) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->nemo_toolkit[nlp]) (1.17.48)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nemo_toolkit[nlp]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nemo_toolkit[nlp]) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nemo_toolkit[nlp]) (2.4.7)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome->nemo_toolkit[nlp]) (7.1.2)\n",
            "Collecting pybind11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/e3/d576f6f02bc75bacbc3d42494e8f1d063c95617d86648dba243c2cb3963e/pybind11-2.5.0-py2.py3-none-any.whl (296kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx>=1.7.0->nemo_toolkit[nlp]) (49.6.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (0.35.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (3.2.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (0.8.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.17.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.31.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core==1.0.0rc4->nemo_toolkit[nlp]) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.11.0->nemo_toolkit[nlp]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.11.0->nemo_toolkit[nlp]) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.11.0->nemo_toolkit[nlp]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.11.0->nemo_toolkit[nlp]) (2.10)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->nemo_toolkit[nlp]) (0.15.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.7.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (3.1.0)\n",
            "Building wheels for collected packages: nemo-toolkit, wget, PyYAML, future, antlr4-python3-runtime, sacremoses\n",
            "  Building wheel for nemo-toolkit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nemo-toolkit: filename=nemo_toolkit-1.0.0a0-cp36-none-any.whl size=366736 sha256=bf5c769d86f110565dfbefb264685ddf1afaba78cd579f4ba80ec7b0debb25a2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_yrkg5m0/wheels/53/0d/1a/5ec64716c0751ea1f1000c1131cb5ed8d573af06685aa69ec0\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=c8be86a076c48563af08511895c7ec4bac7e9264d6e0b0a67a61f4905c34b48d\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=a039848b0c74e7d5b18ec07ed7037db1cd578e9a11e732670e3498237121ffd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=1d37311a288439a0275c1a12d743b0f7863df1c8c302fded51a9222b36a5d1e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141230 sha256=75513e231ed96eda865b2ab020b86dee45cfb57ba7efcc05564ac513981c3dd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=c4ccf43283c946d73f0e8be7d923e650721d7390c19646031ec10c46997a7e1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built nemo-toolkit wget PyYAML future antlr4-python3-runtime sacremoses\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 2.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: onnx, tensorboard, PyYAML, future, pytorch-lightning, wget, ruamel.yaml.clib, ruamel.yaml, omegaconf, importlib-resources, antlr4-python3-runtime, hydra-core, tokenizers, sentencepiece, sacremoses, transformers, unidecode, youtokentome, rapidfuzz, pybind11, megatron-lm, nemo-toolkit\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.3.1 antlr4-python3-runtime-4.8 future-0.18.2 hydra-core-1.0.0rc4 importlib-resources-3.0.0 megatron-lm-1.1.4 nemo-toolkit-1.0.0a0 omegaconf-2.0.1rc12 onnx-1.7.0 pybind11-2.5.0 pytorch-lightning-0.9.0 rapidfuzz-0.11.1 ruamel.yaml-0.16.11 ruamel.yaml.clib-0.2.0 sacremoses-0.0.43 sentencepiece-0.1.91 tensorboard-2.2.0 tokenizers-0.8.1rc2 transformers-3.1.0 unidecode-1.1.1 wget-3.2 youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmvtH0pxHDQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "07594302-25d8-49c4-fc83-b7f46e63011e"
      },
      "source": [
        "import os\n",
        "import wget\n",
        "from nemo.collections import nlp as nemo_nlp\n",
        "from omegaconf import OmegaConf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NeMo W 2020-09-04 04:48:23 experimental:28] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
            "[NeMo W 2020-09-04 04:48:23 experimental:28] Module <class 'nemo.collections.nlp.modules.common.sequence_token_classifier.SequenceTokenClassifier'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: APEX is not installed, multi_tensor_applier will not be available.\n",
            "WARNING: APEX is not installed, using torch.nn.LayerNorm instead of apex.normalization.FusedLayerNorm!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6AARcXXUEbs",
        "colab_type": "text"
      },
      "source": [
        "# Language models\n",
        "\n",
        "Natural Language Processing (NLP) field experienced a huge leap in recent years due to the concept of transfer learning enabled through pretrained language models.\n",
        "\n",
        "[BERT](https://arxiv.org/abs/1810.04805), [RoBERTa](https://arxiv.org/abs/1907.11692), [Megatron-LM](https://arxiv.org/abs/1909.08053), and many other proposed language models achieve state-of-the-art results on many NLP tasks, such as:\n",
        "* question answering\n",
        "* sentiment analysis\n",
        "* named entity recognition and many others.\n",
        "\n",
        "In NeMo, most of the NLP models represent a pretrained language model followed by a Token Classification layer or a Sequence Classification layer or a combination of both. By changing the language model, you can improve the performance of your final model for the specific downstream task you are solving.\n",
        "\n",
        "With NeMo you can use either pretrain a BERT model from your data or use a pretrained language model from [HuggingFace transformers](https://github.com/huggingface/transformers) or [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) libraries.\n",
        "\n",
        "Let's take a look at the list of available pretrained language models:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp7F45bgX7SU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "a7512db4-ba50-4c47-e8f5-addf680a847c"
      },
      "source": [
        "nemo_nlp.modules.get_pretrained_lm_models_list()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['megatron-bert-345m-uncased',\n",
              " 'megatron-bert-345m-cased',\n",
              " 'biomegatron-bert-345m-uncased',\n",
              " 'biomegatron-bert-345m-cased',\n",
              " 'megatron-bert-uncased',\n",
              " 'megatron-bert-cased',\n",
              " 'bert-base-uncased',\n",
              " 'bert-large-uncased',\n",
              " 'bert-base-cased',\n",
              " 'bert-large-cased',\n",
              " 'bert-base-multilingual-uncased',\n",
              " 'bert-base-multilingual-cased',\n",
              " 'bert-base-chinese',\n",
              " 'bert-base-german-cased',\n",
              " 'bert-large-uncased-whole-word-masking',\n",
              " 'bert-large-cased-whole-word-masking',\n",
              " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
              " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
              " 'bert-base-cased-finetuned-mrpc',\n",
              " 'bert-base-german-dbmdz-cased',\n",
              " 'bert-base-german-dbmdz-uncased',\n",
              " 'cl-tohoku/bert-base-japanese',\n",
              " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
              " 'cl-tohoku/bert-base-japanese-char',\n",
              " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
              " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
              " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
              " 'wietsedv/bert-base-dutch-cased',\n",
              " 'distilbert-base-uncased',\n",
              " 'distilbert-base-uncased-distilled-squad',\n",
              " 'distilbert-base-cased',\n",
              " 'distilbert-base-cased-distilled-squad',\n",
              " 'distilbert-base-german-cased',\n",
              " 'distilbert-base-multilingual-cased',\n",
              " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
              " 'roberta-base',\n",
              " 'roberta-large',\n",
              " 'roberta-large-mnli',\n",
              " 'distilroberta-base',\n",
              " 'roberta-base-openai-detector',\n",
              " 'roberta-large-openai-detector',\n",
              " 'albert-base-v1',\n",
              " 'albert-large-v1',\n",
              " 'albert-xlarge-v1',\n",
              " 'albert-xxlarge-v1',\n",
              " 'albert-base-v2',\n",
              " 'albert-large-v2',\n",
              " 'albert-xlarge-v2',\n",
              " 'albert-xxlarge-v2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdpxEiirX8F-",
        "colab_type": "text"
      },
      "source": [
        "All NeMo [NLP models](https://github.com/NVIDIA/NeMo/tree/main/examples/nlp) have an associated config file. As an example, let's examine the config file for the Named Entity Recognition (NER) model (more details about the model and the NER task, could be found [here](https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/Punctuation_and_Capitalization.ipynb))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1gA8PsJ13MJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6e51621-abe7-4642-dd1d-7072e922209c"
      },
      "source": [
        "MODEL_CONFIG = \"token_classification_config.yaml\"\n",
        "\n",
        "# download the model's configuration file \n",
        "if not os.path.exists(MODEL_CONFIG):\n",
        "    print('Downloading config file...')\n",
        "    wget.download('https://raw.githubusercontent.com/NVIDIA/NeMo/' + BRANCH + '/examples/nlp/token_classification/conf/' + MODEL_CONFIG)\n",
        "else:\n",
        "    print ('Config file already exists')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading config file...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX3KmWMvSUQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7831ea47-f61a-440e-c06c-08540cbdc2a5"
      },
      "source": [
        "# this line will print the entire config of the model\n",
        "config = OmegaConf.load(MODEL_CONFIG)\n",
        "print(OmegaConf.to_yaml(config))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainer:\n",
            "  gpus: 1\n",
            "  num_nodes: 1\n",
            "  max_epochs: 5\n",
            "  max_steps: null\n",
            "  accumulate_grad_batches: 1\n",
            "  gradient_clip_val: 0.0\n",
            "  amp_level: O0\n",
            "  precision: 16\n",
            "  distributed_backend: ddp\n",
            "  checkpoint_callback: false\n",
            "  logger: false\n",
            "  row_log_interval: 1\n",
            "  val_check_interval: 1.0\n",
            "  resume_from_checkpoint: null\n",
            "exp_manager:\n",
            "  exp_dir: null\n",
            "  name: token_classification_model\n",
            "  create_tensorboard_logger: true\n",
            "  create_checkpoint_callback: true\n",
            "model:\n",
            "  nemo_path: null\n",
            "  label_ids: null\n",
            "  dataset:\n",
            "    data_dir: ???\n",
            "    class_balancing: null\n",
            "    max_seq_length: 128\n",
            "    pad_label: O\n",
            "    ignore_extra_tokens: false\n",
            "    ignore_start_end: false\n",
            "    use_cache: true\n",
            "    num_workers: 2\n",
            "    pin_memory: false\n",
            "    drop_last: false\n",
            "  train_ds:\n",
            "    text_file: text_train.txt\n",
            "    labels_file: labels_train.txt\n",
            "    shuffle: true\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "  validation_ds:\n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "  language_model:\n",
            "    pretrained_model_name: bert-base-uncased\n",
            "    bert_checkpoint: null\n",
            "    bert_config: null\n",
            "    tokenizer: nemobert\n",
            "    vocab_file: null\n",
            "    tokenizer_model: null\n",
            "    do_lower_case: false\n",
            "  head:\n",
            "    num_fc_layers: 2\n",
            "    fc_dropout: 0.5\n",
            "    activation: relu\n",
            "    log_softmax: true\n",
            "    use_transformer_init: true\n",
            "  optim:\n",
            "    name: adam\n",
            "    lr: 5.0e-05\n",
            "    weight_decay: 0.0\n",
            "    sched:\n",
            "      name: WarmupAnnealing\n",
            "      warmup_steps: null\n",
            "      warmup_ratio: 0.1\n",
            "      last_epoch: -1\n",
            "      monitor: val_loss\n",
            "      reduce_on_plateau: false\n",
            "hydra:\n",
            "  run:\n",
            "    dir: .\n",
            "  job_logging:\n",
            "    root:\n",
            "      handlers: null\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKFbFDwzKUZB",
        "colab_type": "text"
      },
      "source": [
        "For the purposes of this tutorial, we are interested in the language_model part of the Named Entity Recognition Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6hlcCYyKFiY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "56237ed3-5fc9-4b5a-93ee-411871bf2916"
      },
      "source": [
        "print(OmegaConf.to_yaml(config.model.language_model))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained_model_name: bert-base-uncased\n",
            "bert_checkpoint: null\n",
            "bert_config: null\n",
            "tokenizer: nemobert\n",
            "vocab_file: null\n",
            "tokenizer_model: null\n",
            "do_lower_case: false\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xldsDiH9ZY2h",
        "colab_type": "text"
      },
      "source": [
        "There are might be slight differences from one model to another, but most of them have the following important parameters associated with a language model:\n",
        "* `pretrained_model_name` - a name of the pretrained model from either HuggingFace or Megatron-LM libraries\n",
        "* `bert_checkpoint` - a path to the pretrained language model checkpoint if, for example, you trained a BERT model with your own data\n",
        "* `bert_config` - a path the model config file if a language you want to use differs from the model's default configuration\n",
        "\n",
        "To modify the default language model, specify the desired language model name with `model.language_model.pretrained_model_name` argument, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r_1gIXwashY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.model.language_model = 'roberta-base'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVp4zvxPatga",
        "colab_type": "text"
      },
      "source": [
        "and then start the training as usual (please see [tutorials/nlp](https://github.com/NVIDIA/NeMo/tree/main/tutorials/nlp) for more details about training a particular model). \n",
        "\n",
        "You can also provide a pretrained language model checkpoint and a configuration file if available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi_LBdSA4U6Q",
        "colab_type": "text"
      },
      "source": [
        "# Downstream tasks with Megatron and BioMegatron LM\n",
        "\n",
        "All the above holds for both HuggingFace and Megatron-LM pretrained language models, but let's closely examine the Megatron-LM.\n",
        "\n",
        "[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) is a large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA. More details could be found at [Megatron-LM github repo](https://github.com/NVIDIA/Megatron-LM).\n",
        "\n",
        "To see the list of available Megatron-LM models in NeMo, run:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o-XPMrIQBmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1f670398-369c-409b-886c-345c025a281b"
      },
      "source": [
        "nemo_nlp.modules.get_megatron_lm_models_list()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['megatron-bert-345m-uncased',\n",
              " 'megatron-bert-345m-cased',\n",
              " 'biomegatron-bert-345m-uncased',\n",
              " 'biomegatron-bert-345m-cased',\n",
              " 'megatron-bert-uncased',\n",
              " 'megatron-bert-cased']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM_ei7OSLn1X",
        "colab_type": "text"
      },
      "source": [
        "If you want to use one of the available Megatron-LM models, specify its name with `model.language_model.pretrained_model_name` argument, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcQLoouME8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.model.language_model = 'megatron-bert-345m-uncased'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mla-BZexMM2u",
        "colab_type": "text"
      },
      "source": [
        "If you have a different checkpoint or a model configuration file, use these general Megatron-LM model names:\n",
        "* `megatron-bert-uncased` or \n",
        "* `megatron-bert-cased` \n",
        "\n",
        "and provide associated bert_config and bert_checkpoint files, as follows:\n",
        "\n",
        "`model.language_model.pretrained_model_name=megatron-bert-uncased \\\n",
        "model.language_model.bert_checkpoint=<PATH_TO_CHECKPOINT> \\\n",
        "model.language_model.bert_config=<PAHT_TO_CONFIG>`\n",
        " \n",
        " or \n",
        " \n",
        "`model.language_model.pretrained_model_name=megatron-bert-cased \\\n",
        "model.language_model.bert_checkpoint=<PATH_TO_CHECKPOINT> \\\n",
        "model.language_model.bert_config=<PAHT_TO_CONFIG>`\n",
        "\n",
        "The general Megatron-LM model names are used to download the correct vocabulary file needed to setup the model correctly. Note, the data preprocessing and model training is done in NeMo. Megatron-LM has its own set of training arguments (including tokenizer) that are ignored during finetuning in NeMo. Please see downstream task [config files and training scripts](https://github.com/NVIDIA/NeMo/tree/main/examples/nlp) for all NeMo supported arguments.\n",
        "\n",
        "## Download pretrained model\n",
        "\n",
        "With NeMo, the original and domain-specific Megatron-LM BERT models and model configuration files will be downloaded automatically, but they also could be downloaded with the links below:\n",
        "\n",
        "[Megatron-LM BERT Uncased 345M (~345M parameters): https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m](https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m/files?version=v0.1_uncased)\n",
        "\n",
        "[Megatron-LM BERT Cased 345M (~345M parameters): https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m](https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m/files?version=v0.1_cased)\n",
        "\n",
        "[BioMegatron-LM BERT Cased 345M (~345M parameters): https://ngc.nvidia.com/catalog/models/nvidia:biomegatron345mcased](https://ngc.nvidia.com/catalog/models/nvidia:biomegatron345mcased)\n",
        "\n",
        "[BioMegatron-LM BERT Uncased 345M (~345M parameters)](https://ngc.nvidia.com/catalog/models/nvidia:biomegatron345muncased): https://ngc.nvidia.com/catalog/models/nvidia:biomegatron345muncased"
      ]
    }
  ]
}