

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>NeMo ASR collection &mdash; nemo 0.11.0b9 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="NeMo CV collection" href="nemo_cv.html" />
    <link rel="prev" title="NeMo Common Collection" href="core.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nemo
          

          
          </a>

          
            
            
              <div class="version">
                0.11.0b9
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Fast Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asr/intro.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speaker_recognition/intro.html">Speaker Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech_command/intro.html">Speech Commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../voice_activity_detection/intro.html">Voice Activity Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nlp/intro.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tts/intro.html">Speech Synthesis</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">NeMo Collections API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="core.html">NeMo Common Collection</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">NeMo ASR collection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-nemo.collections.asr.data_layer">Speech data processing modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-nemo.collections.asr.jasper">Automatic Speech Recognition modules</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nemo_cv.html">NeMo CV collection</a></li>
<li class="toctree-l2"><a class="reference internal" href="nemo_tts.html">NeMo TTS collection</a></li>
<li class="toctree-l2"><a class="reference internal" href="nemo_nlp.html">NeMo NLP collection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api-docs/modules.html">NeMo API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chinese/intro.html">中文支持</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nemo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">NeMo Collections API</a> &raquo;</li>
        
      <li>NeMo ASR collection</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/collections/nemo_asr.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="nemo-asr-collection">
<h1>NeMo ASR collection<a class="headerlink" href="#nemo-asr-collection" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-nemo.collections.asr.data_layer">
<span id="speech-data-processing-modules"></span><h2>Speech data processing modules<a class="headerlink" href="#module-nemo.collections.asr.data_layer" title="Permalink to this headline">¶</a></h2>
<p>This package contains Neural Modules responsible for ASR data layers.</p>
<dl class="class">
<dt id="nemo.collections.asr.data_layer.AudioToTextDataLayer">
<em class="property">class </em><code class="sig-prename descclassname">nemo.collections.asr.data_layer.</code><code class="sig-name descname">AudioToTextDataLayer</code><span class="sig-paren">(</span><em class="sig-param">manifest_filepath, labels, batch_size, sample_rate=16000, int_values=False, bos_id=None, eos_id=None, pad_id=None, min_duration=0.1, max_duration=None, normalize_transcripts=True, trim_silence=False, load_audio=True, drop_last=False, shuffle=True, num_workers=0, augmentor: Union[nemo.collections.asr.parts.perturb.AudioAugmentor, Dict[str, Dict[str, Any]], None] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nemo/collections/asr/data_layer.html#AudioToTextDataLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nemo.collections.asr.data_layer.AudioToTextDataLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../api-docs/nemo.html#nemo.backends.pytorch.nm.DataLayerNM" title="nemo.backends.pytorch.nm.DataLayerNM"><code class="xref py py-class docutils literal notranslate"><span class="pre">nemo.backends.pytorch.nm.DataLayerNM</span></code></a></p>
<p>Data Layer for general ASR tasks.</p>
<p>Module which reads ASR labeled data. It accepts comma-separated
JSON manifest files describing the correspondence between wav audio files
and their transcripts. JSON files should be of the following format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;audio_filepath&quot;</span><span class="p">:</span> <span class="n">path_to_wav_0</span><span class="p">,</span> <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="n">time_in_sec_0</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">transcript_0</span><span class="p">}</span>
<span class="o">...</span>
<span class="p">{</span><span class="s2">&quot;audio_filepath&quot;</span><span class="p">:</span> <span class="n">path_to_wav_n</span><span class="p">,</span> <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="n">time_in_sec_n</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">transcript_n</span><span class="p">}</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>manifest_filepath</strong> (<em>str</em>) – Dataset parameter.
Path to JSON containing data.</p></li>
<li><p><strong>labels</strong> (<em>list</em>) – Dataset parameter.
List of characters that can be output by the ASR model.
For Jasper, this is the 28 character set {a-z ‘}. The CTC blank
symbol is automatically added later for models using ctc.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – batch size</p></li>
<li><p><strong>sample_rate</strong> (<em>int</em>) – Target sampling rate for data. Audio files will be
resampled to sample_rate if it is not already.
Defaults to 16000.</p></li>
<li><p><strong>int_values</strong> (<em>bool</em>) – Bool indicating whether the audio file is saved as
int data or float data.
Defaults to False.</p></li>
<li><p><strong>bos_id</strong> (<em>id</em>) – Dataset parameter.
Beginning of string symbol id used for seq2seq models.
Defaults to None.</p></li>
<li><p><strong>eos_id</strong> (<em>id</em>) – Dataset parameter.
End of string symbol id used for seq2seq models.
Defaults to None.</p></li>
<li><p><strong>pad_id</strong> (<em>id</em>) – Token used to pad when collating samples in batches.
If this is None, pads using 0s.
Defaults to None.</p></li>
<li><p><strong>min_duration</strong> (<em>float</em>) – Dataset parameter.
All training files which have a duration less than min_duration
are dropped. Note: Duration is read from the manifest JSON.
Defaults to 0.1.</p></li>
<li><p><strong>max_duration</strong> (<em>float</em>) – Dataset parameter.
All training files which have a duration more than max_duration
are dropped. Note: Duration is read from the manifest JSON.
Defaults to None.</p></li>
<li><p><strong>normalize_transcripts</strong> (<em>bool</em>) – Dataset parameter.
Whether to use automatic text cleaning.
It is highly recommended to manually clean text for best results.
Defaults to True.</p></li>
<li><p><strong>trim_silence</strong> (<em>bool</em>) – Whether to use trim silence from beginning and end
of audio signal using librosa.effects.trim().
Defaults to False.</p></li>
<li><p><strong>load_audio</strong> (<em>bool</em>) – Dataset parameter.
Controls whether the dataloader loads the audio signal and
transcript or just the transcript.
Defaults to True.</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – See PyTorch DataLoader.
Defaults to False.</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em>) – See PyTorch DataLoader.
Defaults to True.</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – See PyTorch DataLoader.
Defaults to 0.</p></li>
<li><p><strong>perturb_config</strong> (<em>dict</em>) – Currently disabled.</p></li>
<li><p><strong>augmentor</strong> (<em>AudioAugmentor</em><em> or </em><em>dict</em>) – Optional AudioAugmentor or
dictionary of str -&gt; kwargs (dict) which is parsed and used
to initialize an AudioAugmentor.
Note: It is crucial that each individual augmentation has
a keyword <cite>prob</cite>, that defines a float probability in the
the range [0, 1] of this augmentation being applied.
If this keyword is not present, then the augmentation is
disabled and a warning is logged.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nemo.collections.asr.data_layer.AudioToTextDataLayer.data_iterator">
<em class="property">property </em><code class="sig-name descname">data_iterator</code><a class="headerlink" href="#nemo.collections.asr.data_layer.AudioToTextDataLayer.data_iterator" title="Permalink to this definition">¶</a></dt>
<dd><p>“Iterator over the dataset. It is a good idea to return
torch.utils.data.DataLoader here. Should implement either this or
<cite>dataset</cite>.
If this is implemented, <cite>dataset</cite> property should return None.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.data_layer.AudioToTextDataLayer.dataset">
<em class="property">property </em><code class="sig-name descname">dataset</code><a class="headerlink" href="#nemo.collections.asr.data_layer.AudioToTextDataLayer.dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Should return an instance of torch.utils.data.Dataset. Should
implement
either this or <cite>data_iterator</cite>. If this is implemented, <cite>data_iterator</cite>
should return None.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.data_layer.AudioToTextDataLayer.output_ports">
<em class="property">property </em><code class="sig-name descname">output_ports</code><a class="headerlink" href="#nemo.collections.asr.data_layer.AudioToTextDataLayer.output_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module output ports.</p>
<ul class="simple">
<li><p><em>audio_signal</em> : axes: (batch, time); elements_type: AudioSignal</p></li>
<li><p><em>a_sig_length</em> : axes: (batch,); elements_type: LengthsType</p></li>
<li><p><em>transcripts</em> : axes: (batch, time); elements_type: LabelsType</p></li>
<li><p><em>transcript_length</em> : axes: (batch,); elements_type: LengthsType</p></li>
</ul>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nemo.collections.asr.data_layer.TarredAudioToTextDataLayer">
<em class="property">class </em><code class="sig-prename descclassname">nemo.collections.asr.data_layer.</code><code class="sig-name descname">TarredAudioToTextDataLayer</code><span class="sig-paren">(</span><em class="sig-param">audio_tar_filepaths, manifest_filepath, labels, batch_size, sample_rate=16000, int_values=False, bos_id=None, eos_id=None, pad_id=None, min_duration=0.1, max_duration=None, normalize_transcripts=True, trim_silence=False, shuffle_n=0, num_workers=0, augmentor: Union[nemo.collections.asr.parts.perturb.AudioAugmentor, Dict[str, Dict[str, Any]], None] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nemo/collections/asr/data_layer.html#TarredAudioToTextDataLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nemo.collections.asr.data_layer.TarredAudioToTextDataLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../api-docs/nemo.html#nemo.backends.pytorch.nm.DataLayerNM" title="nemo.backends.pytorch.nm.DataLayerNM"><code class="xref py py-class docutils literal notranslate"><span class="pre">nemo.backends.pytorch.nm.DataLayerNM</span></code></a></p>
<p>Data Layer for general ASR tasks, where the audio files are tarred.</p>
<p>Module which reads ASR labeled data. It accepts a single comma-separated JSON manifest file, as well as the
path(s) to the tarball(s) with the wav files. Each line of the manifest should contain the information for one
audio file, including at least the transcript and name of the audio file (doesn’t have to be exact, only the
basename must be the same).</p>
<p>Valid formats for the audio_tar_filepaths argument include (1) a single string that can be brace-expanded,
e.g. ‘path/to/audio.tar’ or ‘path/to/audio_{1..100}.tar.gz’, or (2) a list of file paths that will not be
brace-expanded, e.g. [‘audio_1.tar’, ‘audio_2.tar’, …]. See the WebDataset documentation for more information
about accepted data and input formats.</p>
<p>If using torch.distributed, the number of shards should be divisible by the number of workers to ensure an
even split among workers. If it is not divisible, logging will give a warning but we will continue.</p>
<p>Notice that a few arguments are different from the AudioToTextDataLayer; for example, shuffle (bool) has been
replaced by shuffle_n (int).</p>
<p>Additionally, please note that the len() of this DataLayer is assumed to be the length of the manifest. Be aware
of this especially if the tarred audio is a subset of the samples represented in the manifest.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>audio_tar_filepaths</strong> – Either a list of audio tarball filepaths, or a
string (can be brace-expandable).</p></li>
<li><p><strong>manifest_filepath</strong> (<em>str</em>) – Path to the manifest.</p></li>
<li><p><strong>labels</strong> (<em>list</em>) – List of characters that can be output by the ASR model.
For Jasper, this is the 28 character set {a-z ‘}. The CTC blank
symbol is automatically added later for models using ctc.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – batch size</p></li>
<li><p><strong>sample_rate</strong> (<em>int</em>) – Target sampling rate for data. Audio files will be
resampled to sample_rate if it is not already.
Defaults to 16000.</p></li>
<li><p><strong>int_values</strong> (<em>bool</em>) – Bool indicating whether the audio file is saved as
int data or float data.
Defaults to False.</p></li>
<li><p><strong>bos_id</strong> (<em>id</em>) – Dataset parameter.
Beginning of string symbol id used for seq2seq models.
Defaults to None.</p></li>
<li><p><strong>eos_id</strong> (<em>id</em>) – Dataset parameter.
End of string symbol id used for seq2seq models.
Defaults to None.</p></li>
<li><p><strong>pad_id</strong> (<em>id</em>) – Token used to pad when collating samples in batches.
If this is None, pads using 0s.
Defaults to None.</p></li>
<li><p><strong>min_duration</strong> (<em>float</em>) – Dataset parameter.
All training files which have a duration less than min_duration
are dropped. Note: Duration is read from the manifest JSON.
Defaults to 0.1.</p></li>
<li><p><strong>max_duration</strong> (<em>float</em>) – Dataset parameter.
All training files which have a duration more than max_duration
are dropped. Note: Duration is read from the manifest JSON.
Defaults to None.</p></li>
<li><p><strong>normalize_transcripts</strong> (<em>bool</em>) – Dataset parameter.
Whether to use automatic text cleaning.
It is highly recommended to manually clean text for best results.
Defaults to True.</p></li>
<li><p><strong>trim_silence</strong> (<em>bool</em>) – Whether to use trim silence from beginning and end
of audio signal using librosa.effects.trim().
Defaults to False.</p></li>
<li><p><strong>shuffle_n</strong> (<em>int</em>) – How many samples to look ahead and load to be shuffled.
See WebDataset documentation for more details.
Defaults to 0.</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – See PyTorch DataLoader. Defaults to 0.</p></li>
<li><p><strong>augmentor</strong> (<em>AudioAugmentor</em><em> or </em><em>dict</em>) – Optional AudioAugmentor or
dictionary of str -&gt; kwargs (dict) which is parsed and used
to initialize an AudioAugmentor.
Note: It is crucial that each individual augmentation has
a keyword <cite>prob</cite>, that defines a float probability in the
the range [0, 1] of this augmentation being applied.
If this keyword is not present, then the augmentation is
disabled and a warning is logged.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nemo.collections.asr.data_layer.TarredAudioToTextDataLayer.data_iterator">
<em class="property">property </em><code class="sig-name descname">data_iterator</code><a class="headerlink" href="#nemo.collections.asr.data_layer.TarredAudioToTextDataLayer.data_iterator" title="Permalink to this definition">¶</a></dt>
<dd><p>“Iterator over the dataset. It is a good idea to return
torch.utils.data.DataLoader here. Should implement either this or
<cite>dataset</cite>.
If this is implemented, <cite>dataset</cite> property should return None.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.data_layer.TarredAudioToTextDataLayer.dataset">
<em class="property">property </em><code class="sig-name descname">dataset</code><a class="headerlink" href="#nemo.collections.asr.data_layer.TarredAudioToTextDataLayer.dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Should return an instance of torch.utils.data.Dataset. Should
implement
either this or <cite>data_iterator</cite>. If this is implemented, <cite>data_iterator</cite>
should return None.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.data_layer.TarredAudioToTextDataLayer.output_ports">
<em class="property">property </em><code class="sig-name descname">output_ports</code><a class="headerlink" href="#nemo.collections.asr.data_layer.TarredAudioToTextDataLayer.output_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module output ports.</p>
<ul class="simple">
<li><p><em>audio_signal</em> : axes: (batch, time); elements_type: AudioSignal</p></li>
<li><p><em>a_sig_length</em> : axes: (batch,); elements_type: LengthsType</p></li>
<li><p><em>transcripts</em> : axes: (batch, time); elements_type: LabelsType</p></li>
<li><p><em>transcript_length</em> : axes: (batch,); elements_type: LengthsType</p></li>
</ul>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nemo.collections.asr.data_layer.KaldiFeatureDataLayer">
<em class="property">class </em><code class="sig-prename descclassname">nemo.collections.asr.data_layer.</code><code class="sig-name descname">KaldiFeatureDataLayer</code><span class="sig-paren">(</span><em class="sig-param">kaldi_dir</em>, <em class="sig-param">labels</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">min_duration=None</em>, <em class="sig-param">max_duration=None</em>, <em class="sig-param">normalize_transcripts=True</em>, <em class="sig-param">drop_last=False</em>, <em class="sig-param">shuffle=True</em>, <em class="sig-param">num_workers=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nemo/collections/asr/data_layer.html#KaldiFeatureDataLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nemo.collections.asr.data_layer.KaldiFeatureDataLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../api-docs/nemo.html#nemo.backends.pytorch.nm.DataLayerNM" title="nemo.backends.pytorch.nm.DataLayerNM"><code class="xref py py-class docutils literal notranslate"><span class="pre">nemo.backends.pytorch.nm.DataLayerNM</span></code></a></p>
<p>Data layer for reading generic Kaldi-formatted data.</p>
<p>Module that reads ASR labeled data that is in a Kaldi-compatible format.
It assumes that you have a directory that contains:</p>
<ul class="simple">
<li><dl class="simple">
<dt>feats.scp: A mapping from utterance IDs to .ark files that</dt><dd><p>contains the corresponding MFCC (or other format) data</p>
</dd>
</dl>
</li>
<li><p>text: A mapping from utterance IDs to transcripts</p></li>
<li><dl class="simple">
<dt>utt2dur (optional): A mapping from utterance IDs to audio durations,</dt><dd><p>needed if you want to filter based on duration</p>
</dd>
</dl>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kaldi_dir</strong> (<em>str</em>) – Directory that contains the above files.</p></li>
<li><p><strong>labels</strong> (<em>list</em>) – List of characters that can be output by the ASR model,
e.g. {a-z ‘} for Jasper. The CTC blank symbol is automatically
added later for models using CTC.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – batch size</p></li>
<li><p><strong>eos_id</strong> (<em>str</em>) – End of string symbol used for seq2seq models.
Defaults to None.</p></li>
<li><p><strong>min_duration</strong> (<em>float</em>) – All training files which have a duration less
than min_duration are dropped. Can’t be used if the <cite>utt2dur</cite> file
does not exist. Defaults to None.</p></li>
<li><p><strong>max_duration</strong> (<em>float</em>) – All training files which have a duration more
than max_duration are dropped. Can’t be used if the <cite>utt2dur</cite> file
does not exist. Defaults to None.</p></li>
<li><p><strong>normalize_transcripts</strong> (<em>bool</em>) – Whether to use automatic text cleaning.
It is highly recommended to manually clean text for best results.
Defaults to True.</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – See PyTorch DataLoader. Defaults to False.</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em>) – See PyTorch DataLoader. Defaults to True.</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – See PyTorch DataLoader. Defaults to 0.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nemo.collections.asr.data_layer.KaldiFeatureDataLayer.data_iterator">
<em class="property">property </em><code class="sig-name descname">data_iterator</code><a class="headerlink" href="#nemo.collections.asr.data_layer.KaldiFeatureDataLayer.data_iterator" title="Permalink to this definition">¶</a></dt>
<dd><p>“Iterator over the dataset. It is a good idea to return
torch.utils.data.DataLoader here. Should implement either this or
<cite>dataset</cite>.
If this is implemented, <cite>dataset</cite> property should return None.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.data_layer.KaldiFeatureDataLayer.dataset">
<em class="property">property </em><code class="sig-name descname">dataset</code><a class="headerlink" href="#nemo.collections.asr.data_layer.KaldiFeatureDataLayer.dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Should return an instance of torch.utils.data.Dataset. Should
implement
either this or <cite>data_iterator</cite>. If this is implemented, <cite>data_iterator</cite>
should return None.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.data_layer.KaldiFeatureDataLayer.output_ports">
<em class="property">property </em><code class="sig-name descname">output_ports</code><a class="headerlink" href="#nemo.collections.asr.data_layer.KaldiFeatureDataLayer.output_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module output ports.</p>
<ul class="simple">
<li><p><em>processed_signal</em> : axes: (batch, dimension, time); elements_type: SpectrogramType</p></li>
<li><p><em>processed_length</em> : axes: (batch,); elements_type: LengthsType</p></li>
<li><p><em>transcripts</em> : axes: (batch, time); elements_type: LabelsType</p></li>
<li><p><em>transcript_length</em> : axes: (batch,); elements_type: LengthsType</p></li>
</ul>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nemo.collections.asr.data_layer.TranscriptDataLayer">
<em class="property">class </em><code class="sig-prename descclassname">nemo.collections.asr.data_layer.</code><code class="sig-name descname">TranscriptDataLayer</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">labels</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">bos_id=None</em>, <em class="sig-param">eos_id=None</em>, <em class="sig-param">pad_id=None</em>, <em class="sig-param">drop_last=False</em>, <em class="sig-param">num_workers=0</em>, <em class="sig-param">shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nemo/collections/asr/data_layer.html#TranscriptDataLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nemo.collections.asr.data_layer.TranscriptDataLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../api-docs/nemo.html#nemo.backends.pytorch.nm.DataLayerNM" title="nemo.backends.pytorch.nm.DataLayerNM"><code class="xref py py-class docutils literal notranslate"><span class="pre">nemo.backends.pytorch.nm.DataLayerNM</span></code></a></p>
<p>A simple Neural Module for loading textual transcript data.
The path, labels, and eos_id arguments are dataset parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pad_id</strong> (<em>int</em>) – Label position of padding symbol</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Size of batches to generate in data loader</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – Whether we drop last (possibly) incomplete batch.
Defaults to False.</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of processes to work on data loading (0 for
just main process).
Defaults to 0.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nemo.collections.asr.data_layer.TranscriptDataLayer.data_iterator">
<em class="property">property </em><code class="sig-name descname">data_iterator</code><a class="headerlink" href="#nemo.collections.asr.data_layer.TranscriptDataLayer.data_iterator" title="Permalink to this definition">¶</a></dt>
<dd><p>“Iterator over the dataset. It is a good idea to return
torch.utils.data.DataLoader here. Should implement either this or
<cite>dataset</cite>.
If this is implemented, <cite>dataset</cite> property should return None.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.data_layer.TranscriptDataLayer.dataset">
<em class="property">property </em><code class="sig-name descname">dataset</code><a class="headerlink" href="#nemo.collections.asr.data_layer.TranscriptDataLayer.dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Should return an instance of torch.utils.data.Dataset. Should
implement
either this or <cite>data_iterator</cite>. If this is implemented, <cite>data_iterator</cite>
should return None.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.data_layer.TranscriptDataLayer.output_ports">
<em class="property">property </em><code class="sig-name descname">output_ports</code><a class="headerlink" href="#nemo.collections.asr.data_layer.TranscriptDataLayer.output_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module output ports.</p>
<dl>
<dt>texts:</dt><dd><p>0: AxisType(BatchTag)</p>
<p>1: AxisType(TimeTag)</p>
</dd>
<dt>texts_length:</dt><dd><p>0: AxisType(BatchTag)</p>
</dd>
</dl>
<ul class="simple">
<li><p><em>texts</em> : axes: (batch, time); elements_type: LabelsType</p></li>
<li><p><em>texts_length</em> : axes: (batch,); elements_type: LengthsType</p></li>
</ul>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nemo.collections.asr.data_layer.AudioToSpeechLabelDataLayer">
<em class="property">class </em><code class="sig-prename descclassname">nemo.collections.asr.data_layer.</code><code class="sig-name descname">AudioToSpeechLabelDataLayer</code><span class="sig-paren">(</span><em class="sig-param">*, manifest_filepath: str, labels: List[str], batch_size: int, sample_rate: int = 16000, int_values: bool = False, num_workers: int = 0, shuffle: bool = True, min_duration: Optional[float] = 0.1, max_duration: Optional[float] = None, trim_silence: bool = False, drop_last: bool = False, load_audio: bool = True, augmentor: Union[nemo.collections.asr.parts.perturb.AudioAugmentor, Dict[str, Dict[str, Any]], None] = None, time_length: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nemo/collections/asr/data_layer.html#AudioToSpeechLabelDataLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nemo.collections.asr.data_layer.AudioToSpeechLabelDataLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../api-docs/nemo.html#nemo.backends.pytorch.nm.DataLayerNM" title="nemo.backends.pytorch.nm.DataLayerNM"><code class="xref py py-class docutils literal notranslate"><span class="pre">nemo.backends.pytorch.nm.DataLayerNM</span></code></a></p>
<p>Data Layer for general speech classification.</p>
<p>Module which reads speech recognition with target label. It accepts comma-separated
JSON manifest files describing the correspondence between wav audio files
and their target labels. JSON files should be of the following format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;audio_filepath&quot;</span><span class="p">:</span> <span class="n">path_to_wav_0</span><span class="p">,</span> <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="n">time_in_sec_0</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">target_label_0</span><span class="p">,</span> <span class="s2">&quot;offset&quot;</span><span class="p">:</span> <span class="n">offset_in_sec_0</span><span class="p">}</span>
<span class="o">...</span>
<span class="p">{</span><span class="s2">&quot;audio_filepath&quot;</span><span class="p">:</span> <span class="n">path_to_wav_n</span><span class="p">,</span> <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="n">time_in_sec_n</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">target_label_n</span><span class="p">,</span> <span class="s2">&quot;offset&quot;</span><span class="p">:</span> <span class="n">offset_in_sec_n</span><span class="p">}</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>manifest_filepath</strong> (<em>str</em>) – Dataset parameter.
Path to JSON containing data.</p></li>
<li><p><strong>labels</strong> (<em>list</em>) – Dataset parameter.
List of target classes that can be output by the speech recognition model.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – batch size</p></li>
<li><p><strong>sample_rate</strong> (<em>int</em>) – Target sampling rate for data. Audio files will be
resampled to sample_rate if it is not already.
Defaults to 16000.</p></li>
<li><p><strong>int_values</strong> (<em>bool</em>) – Bool indicating whether the audio file is saved as
int data or float data.
Defaults to False.</p></li>
<li><p><strong>min_duration</strong> (<em>float</em>) – Dataset parameter.
All training files which have a duration less than min_duration
are dropped. Note: Duration is read from the manifest JSON.
Defaults to 0.1.</p></li>
<li><p><strong>max_duration</strong> (<em>float</em>) – Dataset parameter.
All training files which have a duration more than max_duration
are dropped. Note: Duration is read from the manifest JSON.
Defaults to None.</p></li>
<li><p><strong>trim_silence</strong> (<em>bool</em>) – Whether to use trim silence from beginning and end
of audio signal using librosa.effects.trim().
Defaults to False.</p></li>
<li><p><strong>load_audio</strong> (<em>bool</em>) – Dataset parameter.
Controls whether the dataloader loads the audio signal and
transcript or just the transcript.
Defaults to True.</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – See PyTorch DataLoader.
Defaults to False.</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em>) – See PyTorch DataLoader.
Defaults to True.</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – See PyTorch DataLoader.
Defaults to 0.</p></li>
<li><p><strong>augmenter</strong> (<em>AudioAugmentor</em><em> or </em><em>dict</em>) – Optional AudioAugmentor or
dictionary of str -&gt; kwargs (dict) which is parsed and used
to initialize an AudioAugmentor.
Note: It is crucial that each individual augmentation has
a keyword <cite>prob</cite>, that defines a float probability in the
the range [0, 1] of this augmentation being applied.
If this keyword is not present, then the augmentation is
disabled and a warning is logged.</p></li>
<li><p><strong>time_length</strong> (<em>int</em>) – max seconds to consider in a batch # Pass this only for speaker recognition task</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nemo.collections.asr.data_layer.AudioToSpeechLabelDataLayer.data_iterator">
<em class="property">property </em><code class="sig-name descname">data_iterator</code><a class="headerlink" href="#nemo.collections.asr.data_layer.AudioToSpeechLabelDataLayer.data_iterator" title="Permalink to this definition">¶</a></dt>
<dd><p>“Iterator over the dataset. It is a good idea to return
torch.utils.data.DataLoader here. Should implement either this or
<cite>dataset</cite>.
If this is implemented, <cite>dataset</cite> property should return None.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.data_layer.AudioToSpeechLabelDataLayer.dataset">
<em class="property">property </em><code class="sig-name descname">dataset</code><a class="headerlink" href="#nemo.collections.asr.data_layer.AudioToSpeechLabelDataLayer.dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Should return an instance of torch.utils.data.Dataset. Should
implement
either this or <cite>data_iterator</cite>. If this is implemented, <cite>data_iterator</cite>
should return None.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.data_layer.AudioToSpeechLabelDataLayer.output_ports">
<em class="property">property </em><code class="sig-name descname">output_ports</code><a class="headerlink" href="#nemo.collections.asr.data_layer.AudioToSpeechLabelDataLayer.output_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module output ports.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nemo.collections.asr.jasper">
<span id="automatic-speech-recognition-modules"></span><h2>Automatic Speech Recognition modules<a class="headerlink" href="#module-nemo.collections.asr.jasper" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nemo.collections.asr.jasper.JasperDecoderForCTC">
<em class="property">class </em><code class="sig-prename descclassname">nemo.collections.asr.jasper.</code><code class="sig-name descname">JasperDecoderForCTC</code><span class="sig-paren">(</span><em class="sig-param">feat_in</em>, <em class="sig-param">num_classes</em>, <em class="sig-param">init_mode='xavier_uniform'</em>, <em class="sig-param">vocabulary=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nemo/collections/asr/jasper.html#JasperDecoderForCTC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForCTC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../api-docs/nemo.html#nemo.backends.pytorch.nm.TrainableNM" title="nemo.backends.pytorch.nm.TrainableNM"><code class="xref py py-class docutils literal notranslate"><span class="pre">nemo.backends.pytorch.nm.TrainableNM</span></code></a></p>
<p>Jasper Decoder creates the final layer in Jasper that maps from the outputs
of Jasper Encoder to the vocabulary of interest.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>feat_in</strong> (<em>int</em>) – Number of channels being input to this module</p></li>
<li><p><strong>num_classes</strong> (<em>int</em>) – Number of characters in ASR model’s vocab/labels.
This count should not include the CTC blank symbol.</p></li>
<li><p><strong>init_mode</strong> (<em>str</em>) – Describes how neural network parameters are
initialized. Options are [‘xavier_uniform’, ‘xavier_normal’,
‘kaiming_uniform’,’kaiming_normal’].
Defaults to “xavier_uniform”.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nemo.collections.asr.jasper.JasperDecoderForCTC.input_ports">
<em class="property">property </em><code class="sig-name descname">input_ports</code><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForCTC.input_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module input ports.</p>
<ul class="simple">
<li><p><em>encoder_output</em> : axes: (batch, dimension, time); elements_type: AcousticEncodedRepresentation</p></li>
</ul>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.jasper.JasperDecoderForCTC.output_ports">
<em class="property">property </em><code class="sig-name descname">output_ports</code><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForCTC.output_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module output ports.</p>
<ul class="simple">
<li><p><em>output</em> : axes: (batch, time, dimension); elements_type: LogprobsType</p></li>
</ul>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.jasper.JasperDecoderForCTC.vocabulary">
<em class="property">property </em><code class="sig-name descname">vocabulary</code><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForCTC.vocabulary" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nemo.collections.asr.jasper.JasperDecoderForClassification">
<em class="property">class </em><code class="sig-prename descclassname">nemo.collections.asr.jasper.</code><code class="sig-name descname">JasperDecoderForClassification</code><span class="sig-paren">(</span><em class="sig-param">*</em>, <em class="sig-param">feat_in</em>, <em class="sig-param">num_classes</em>, <em class="sig-param">init_mode='xavier_uniform'</em>, <em class="sig-param">return_logits=True</em>, <em class="sig-param">pooling_type='avg'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nemo/collections/asr/jasper.html#JasperDecoderForClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForClassification" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../api-docs/nemo.html#nemo.backends.pytorch.nm.TrainableNM" title="nemo.backends.pytorch.nm.TrainableNM"><code class="xref py py-class docutils literal notranslate"><span class="pre">nemo.backends.pytorch.nm.TrainableNM</span></code></a></p>
<p>Jasper Decoder creates the final layer in Jasper that maps from the outputs
of Jasper Encoder to one class label.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>feat_in</strong> (<em>int</em>) – Number of channels being input to this module</p></li>
<li><p><strong>num_classes</strong> (<em>int</em>) – Number of characters in ASR model’s vocab/labels.
This count should not include the CTC blank symbol.</p></li>
<li><p><strong>init_mode</strong> (<em>str</em>) – Describes how neural network parameters are
initialized. Options are [‘xavier_uniform’, ‘xavier_normal’,
‘kaiming_uniform’,’kaiming_normal’].
Defaults to “xavier_uniform”.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nemo.collections.asr.jasper.JasperDecoderForClassification.input_ports">
<em class="property">property </em><code class="sig-name descname">input_ports</code><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForClassification.input_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module input ports.</p>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.jasper.JasperDecoderForClassification.output_ports">
<em class="property">property </em><code class="sig-name descname">output_ports</code><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForClassification.output_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module output ports.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nemo.collections.asr.jasper.JasperDecoderForSpkrClass">
<em class="property">class </em><code class="sig-prename descclassname">nemo.collections.asr.jasper.</code><code class="sig-name descname">JasperDecoderForSpkrClass</code><span class="sig-paren">(</span><em class="sig-param">feat_in, num_classes, emb_sizes=[1024, 1024], pool_mode='xvector', init_mode='xavier_uniform'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nemo/collections/asr/jasper.html#JasperDecoderForSpkrClass"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForSpkrClass" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../api-docs/nemo.html#nemo.backends.pytorch.nm.TrainableNM" title="nemo.backends.pytorch.nm.TrainableNM"><code class="xref py py-class docutils literal notranslate"><span class="pre">nemo.backends.pytorch.nm.TrainableNM</span></code></a></p>
<p>Jasper Decoder creates the final layer in Jasper that maps from the outputs
of Jasper Encoder to the embedding layer followed by speaker based softmax loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>feat_in</strong> (<em>int</em>) – Number of channels being input to this module</p></li>
<li><p><strong>num_classes</strong> (<em>int</em>) – Number of unique speakers in dataset</p></li>
<li><p><strong>emb_sizes</strong> (<em>list</em>) – shapes of intermediate embedding layers (we consider speaker embbeddings from 1st of this layers)
Defaults to [1024,1024]</p></li>
<li><p><strong>pool_mode</strong> (<em>str</em>) – Pooling stratergy type. options are ‘gram’,’xvector’,’superVector’.
Defaults to ‘xvector’</p></li>
<li><p><strong>init_mode</strong> (<em>str</em>) – Describes how neural network parameters are
initialized. Options are [‘xavier_uniform’, ‘xavier_normal’,
‘kaiming_uniform’,’kaiming_normal’].
Defaults to “xavier_uniform”.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nemo.collections.asr.jasper.JasperDecoderForSpkrClass.affineLayer">
<code class="sig-name descname">affineLayer</code><span class="sig-paren">(</span><em class="sig-param">inp_shape</em>, <em class="sig-param">out_shape</em>, <em class="sig-param">learn_mean=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nemo/collections/asr/jasper.html#JasperDecoderForSpkrClass.affineLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForSpkrClass.affineLayer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.jasper.JasperDecoderForSpkrClass.input_ports">
<em class="property">property </em><code class="sig-name descname">input_ports</code><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForSpkrClass.input_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module input ports.</p>
<dl>
<dt>encoder_output:</dt><dd><p>0: AxisType(BatchTag)</p>
<p>1: AxisType(EncodedRepresentationTag)</p>
<p>2: AxisType(ProcessedTimeTag)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.jasper.JasperDecoderForSpkrClass.output_ports">
<em class="property">property </em><code class="sig-name descname">output_ports</code><a class="headerlink" href="#nemo.collections.asr.jasper.JasperDecoderForSpkrClass.output_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module output ports.</p>
<dl>
<dt>logits:</dt><dd><p>0: AxisType(BatchTag)</p>
<p>1: AxisType(ChannelTag)</p>
</dd>
<dt>embs:</dt><dd><p>0: AxisType(BatchTag)
1: AxisType(EncodedRepresentationTah)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nemo.collections.asr.jasper.JasperEncoder">
<em class="property">class </em><code class="sig-prename descclassname">nemo.collections.asr.jasper.</code><code class="sig-name descname">JasperEncoder</code><span class="sig-paren">(</span><em class="sig-param">jasper</em>, <em class="sig-param">activation</em>, <em class="sig-param">feat_in</em>, <em class="sig-param">normalization_mode='batch'</em>, <em class="sig-param">residual_mode='add'</em>, <em class="sig-param">norm_groups=-1</em>, <em class="sig-param">conv_mask=True</em>, <em class="sig-param">frame_splicing=1</em>, <em class="sig-param">init_mode='xavier_uniform'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nemo/collections/asr/jasper.html#JasperEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nemo.collections.asr.jasper.JasperEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../api-docs/nemo.html#nemo.backends.pytorch.nm.TrainableNM" title="nemo.backends.pytorch.nm.TrainableNM"><code class="xref py py-class docutils literal notranslate"><span class="pre">nemo.backends.pytorch.nm.TrainableNM</span></code></a></p>
<p>Jasper Encoder creates the pre-processing (prologue), Jasper convolution
block, and the first 3 post-processing (epilogue) layers as described in
Jasper (<a class="reference external" href="https://arxiv.org/abs/1904.03288">https://arxiv.org/abs/1904.03288</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>jasper</strong> (<em>list</em>) – <p>A list of dictionaries. Each element in the list
represents the configuration of one Jasper Block. Each element
should contain:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="c1"># Required parameters</span>
    <span class="s1">&#39;filters&#39;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># Number of output channels,</span>
    <span class="s1">&#39;repeat&#39;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># Number of sub-blocks,</span>
    <span class="s1">&#39;kernel&#39;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># Size of conv kernel,</span>
    <span class="s1">&#39;stride&#39;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># Conv stride</span>
    <span class="s1">&#39;dilation&#39;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># Conv dilation</span>
    <span class="s1">&#39;dropout&#39;</span> <span class="p">(</span><span class="nb">float</span><span class="p">)</span> <span class="c1"># Dropout probability</span>
    <span class="s1">&#39;residual&#39;</span> <span class="p">(</span><span class="nb">bool</span><span class="p">)</span> <span class="c1"># Whether to use residual or not.</span>
    <span class="c1"># Optional parameters</span>
    <span class="s1">&#39;residual_dense&#39;</span> <span class="p">(</span><span class="nb">bool</span><span class="p">)</span> <span class="c1"># Whether to use Dense Residuals</span>
        <span class="c1"># or not. &#39;residual&#39; must be True for &#39;residual_dense&#39;</span>
        <span class="c1"># to be enabled.</span>
        <span class="c1"># Defaults to False.</span>
    <span class="s1">&#39;separable&#39;</span> <span class="p">(</span><span class="nb">bool</span><span class="p">)</span> <span class="c1"># Whether to use separable convolutions.</span>
        <span class="c1"># Defaults to False</span>
    <span class="s1">&#39;groups&#39;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># Number of groups in each conv layer.</span>
        <span class="c1"># Defaults to 1</span>
    <span class="s1">&#39;heads&#39;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># Sharing of separable filters</span>
        <span class="c1"># Defaults to -1</span>
    <span class="s1">&#39;tied&#39;</span> <span class="p">(</span><span class="nb">bool</span><span class="p">)</span>  <span class="c1"># Whether to use the same weights for all</span>
        <span class="c1"># sub-blocks.</span>
        <span class="c1"># Defaults to False</span>
    <span class="s1">&#39;se&#39;</span> <span class="p">(</span><span class="nb">bool</span><span class="p">)</span>  <span class="c1"># Whether to add Squeeze and Excitation</span>
        <span class="c1"># sub-blocks.</span>
        <span class="c1"># Defaults to False</span>
    <span class="s1">&#39;se_reduction_ratio&#39;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># The reduction ratio of the Squeeze</span>
        <span class="c1"># sub-module.</span>
        <span class="c1"># Must be an integer &gt; 1.</span>
        <span class="c1"># Defaults to 8.</span>
    <span class="s1">&#39;se_context_window&#39;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># The size of the temporal context</span>
        <span class="c1"># provided to SE sub-module.</span>
        <span class="c1"># Must be an integer. If value &lt;= 0, will perform global</span>
        <span class="c1"># temporal pooling (global context).</span>
        <span class="c1"># If value &gt;= 1, will perform stride 1 average pooling to</span>
        <span class="c1"># compute context window.</span>
    <span class="s1">&#39;se_interpolation_mode&#39;</span> <span class="p">(</span><span class="nb">str</span><span class="p">)</span> <span class="c1"># Interpolation mode of timestep dimension.</span>
        <span class="c1"># Used only if context window is &gt; 1.</span>
        <span class="c1"># The modes available for resizing are: `nearest`, `linear` (3D-only),</span>
        <span class="c1"># `bilinear`, `area`</span>
    <span class="s1">&#39;kernel_size_factor&#39;</span> <span class="p">(</span><span class="nb">float</span><span class="p">)</span>  <span class="c1"># Conv kernel size multiplier</span>
        <span class="c1"># Can be either an int or float</span>
        <span class="c1"># Kernel size is recomputed as below:</span>
        <span class="c1"># new_kernel_size = int(max(1, (kernel_size * kernel_width)))</span>
        <span class="c1"># to prevent kernel sizes than 1.</span>
        <span class="c1"># Note: If rescaled kernel size is an even integer,</span>
        <span class="c1"># adds 1 to the rescaled kernel size to allow &quot;same&quot;</span>
        <span class="c1"># padding.</span>
    <span class="s1">&#39;stride_last&#39;</span> <span class="p">(</span><span class="nb">bool</span><span class="p">)</span> <span class="c1"># Bool flag to determine whether each</span>
        <span class="c1"># of the the repeated sub-blockss will perform a stride,</span>
        <span class="c1"># or only the last sub-block will perform a strided convolution.</span>
<span class="p">}</span>
</pre></div>
</div>
</p></li>
<li><p><strong>activation</strong> (<em>str</em>) – Activation function used for each sub-blocks. Can be
one of [“hardtanh”, “relu”, “selu”, “swish”].</p></li>
<li><p><strong>feat_in</strong> (<em>int</em>) – Number of channels being input to this module</p></li>
<li><p><strong>normalization_mode</strong> (<em>str</em>) – Normalization to be used in each sub-block.
Can be one of [“batch”, “layer”, “instance”, “group”]
Defaults to “batch”.</p></li>
<li><p><strong>residual_mode</strong> (<em>str</em>) – Type of residual connection.
Can be “add”, “stride_add” or “max”.
“stride_add” mode performs strided convolution prior to residual
addition.
Defaults to “add”.</p></li>
<li><p><strong>norm_groups</strong> (<em>int</em>) – Number of groups for “group” normalization type.
If set to -1, number of channels is used.
Defaults to -1.</p></li>
<li><p><strong>conv_mask</strong> (<em>bool</em>) – Controls the use of sequence length masking prior
to convolutions.
Defaults to True.</p></li>
<li><p><strong>frame_splicing</strong> (<em>int</em>) – Defaults to 1.</p></li>
<li><p><strong>init_mode</strong> (<em>str</em>) – Describes how neural network parameters are
initialized. Options are [‘xavier_uniform’, ‘xavier_normal’,
‘kaiming_uniform’,’kaiming_normal’].
Defaults to “xavier_uniform”.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nemo.collections.asr.jasper.JasperEncoder.input_ports">
<em class="property">property </em><code class="sig-name descname">input_ports</code><a class="headerlink" href="#nemo.collections.asr.jasper.JasperEncoder.input_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module input ports.</p>
<ul class="simple">
<li><p><em>audio_signal</em> : axes: (batch, dimension, time); elements_type: SpectrogramType</p></li>
<li><p><em>length</em> : axes: (batch,); elements_type: LengthsType</p></li>
</ul>
</dd></dl>

<dl class="method">
<dt id="nemo.collections.asr.jasper.JasperEncoder.output_ports">
<em class="property">property </em><code class="sig-name descname">output_ports</code><a class="headerlink" href="#nemo.collections.asr.jasper.JasperEncoder.output_ports" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns definitions of module output ports.</p>
<ul class="simple">
<li><p><em>outputs</em> : axes: (batch, dimension, time); elements_type: AcousticEncodedRepresentation</p></li>
<li><p><em>encoded_lengths</em> : axes: (batch,); elements_type: LengthsType</p></li>
</ul>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="nemo_cv.html" class="btn btn-neutral float-right" title="NeMo CV collection" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="core.html" class="btn btn-neutral float-left" title="NeMo Common Collection" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2020, NVIDIA

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>