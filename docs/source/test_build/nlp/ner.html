

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial &mdash; nemo 0.11.0b9 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorial" href="punctuation.html" />
    <link rel="prev" title="Tutorial" href="text_classification.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nemo
          

          
          </a>

          
            
            
              <div class="version">
                0.11.0b9
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Fast Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asr/intro.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speaker_recognition/intro.html">Speaker Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech_command/intro.html">Speech Commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../voice_activity_detection/intro.html">Voice Activity Detection</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="intro.html">Natural Language Processing</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html#neural-machine-translation-nmt">Neural Machine Translation (NMT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#pretraining-bert">Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#transformer-language-model">Transformer Language Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#megatron-lm-for-downstream-tasks">Megatron-LM for Downstream tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#glue-benchmark">GLUE Benchmark</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#intent-and-slot-filling">Intent and Slot filling</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#text-classification">Text Classification</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="intro.html#named-entity-recognition">Named Entity Recognition</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#download-dataset">Download Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-and-inference-scripts">Training and inference scripts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bioner">BioNER</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-other-bert-models">Using Other BERT Models</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#punctuation-and-word-capitalization">Punctuation and Word Capitalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#question-answering">Question Answering</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#dialogue-state-tracking">Dialogue State Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#asr-postprocessing-with-bert">ASR Postprocessing with BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tts/intro.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../collections/modules.html">NeMo Collections API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-docs/modules.html">NeMo API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chinese/intro.html">中文支持</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nemo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="intro.html">Natural Language Processing</a> &raquo;</li>
        
      <li>Tutorial</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/nlp/ner.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tutorial">
<span id="ner"></span><h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p>Make sure you have <code class="docutils literal notranslate"><span class="pre">nemo</span></code> and <code class="docutils literal notranslate"><span class="pre">nemo_nlp</span></code> installed before starting this
tutorial. See the <a class="reference internal" href="../index.html#installation"><span class="std std-ref">Getting started</span></a> section for more details.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For pretraining BERT in NeMo and pretrained model checkpoints go to <a class="reference external" href="https://nvidia.github.io/NeMo/nlp/bert_pretraining.html">BERT pretraining</a>.</p>
</div>
<div class="section" id="introduction">
<span id="ner-tutorial"></span><h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>This tutorial explains how to implement named entity recognition (NER) in NeMo. We’ll show how to do this with a pre-trained BERT model, or with one that you trained yourself! For more details, check out our BERT pretraining tutorial.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We recommend you try this out in a Jupyter notebook. It’ll make debugging much easier!
See examples/nlp/token_classification/NERWithBERT.ipynb.
All code used in this tutorial is based on <a class="reference internal" href="#ner-scripts"><span class="std std-ref">Training and inference scripts</span></a>.</p>
</div>
</div>
<div class="section" id="download-dataset">
<h2>Download Dataset<a class="headerlink" href="#download-dataset" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://www.clips.uantwerpen.be/conll2003/ner/">CoNLL-2003</a> is a standard evaluation dataset for NER, but any NER dataset will work. The only requirement is that the data is splitted into 2 files: text.txt and labels.txt. The text.txt files should be formatted like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Jennifer</span> <span class="ow">is</span> <span class="kn">from</span> <span class="nn">New</span> <span class="n">York</span> <span class="n">City</span> <span class="o">.</span>
<span class="n">She</span> <span class="n">likes</span> <span class="o">...</span>
<span class="o">...</span>
</pre></div>
</div>
<p>The labels.txt files should be formatted like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">B</span><span class="o">-</span><span class="n">PER</span> <span class="n">O</span> <span class="n">O</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span> <span class="n">I</span><span class="o">-</span><span class="n">LOC</span> <span class="n">I</span><span class="o">-</span><span class="n">LOC</span> <span class="n">O</span>
<span class="n">O</span> <span class="n">O</span> <span class="o">...</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Each line of the text.txt file contains text sequences, where words are separated with spaces. The labels.txt file contains corresponding labels for each word in text.txt, the labels are separated with spaces. Each line of the files should follow the format: [WORD] [SPACE] [WORD] [SPACE] [WORD] (for text.txt) and [LABEL] [SPACE] [LABEL] [SPACE] [LABEL] (for labels.txt).</p>
<p>You can use <a class="reference external" href="https://github.com/NVIDIA/NeMo/tree/master/examples/nlp/token_classification/import_from_iob_format.py">this</a> to convert CoNLL-2003 dataset to the format required for training.</p>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<p>First, we need to create our neural factory with the supported backend. How you should define it depends on whether you’d like to multi-GPU or mixed-precision training. This tutorial assumes that you’re training on one GPU, without mixed precision (<code class="docutils literal notranslate"><span class="pre">optimization_level=&quot;O0&quot;</span></code>). If you want to use mixed precision, set <code class="docutils literal notranslate"><span class="pre">optimization_level</span></code> to <code class="docutils literal notranslate"><span class="pre">O1</span></code> or <code class="docutils literal notranslate"><span class="pre">O2</span></code>.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">WORK_DIR</span> <span class="o">=</span> <span class="s2">&quot;path_to_output_dir&quot;</span>
<span class="n">nf</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">NeuralModuleFactory</span><span class="p">(</span><span class="n">local_rank</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                                   <span class="n">optimization_level</span><span class="o">=</span><span class="s2">&quot;O0&quot;</span><span class="p">,</span>
                                   <span class="n">log_dir</span><span class="o">=</span><span class="n">WORK_DIR</span><span class="p">,</span>
                                   <span class="n">create_tb_writer</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Next, we’ll need to define our tokenizer and our BERT model. There are a couple of different ways you can do this. Keep in mind that NER benefits from casing (“New York City” is easier to identify than “new york city”), so we recommend you use cased models.</p>
<p>If you’re using a standard BERT model, you should do it as follows. To see the full list of BERT model names, check out <code class="docutils literal notranslate"><span class="pre">nemo_nlp.nm.trainables.get_pretrained_lm_models_list()</span></code></p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bert_model</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">trainables</span><span class="o">.</span><span class="n">get_pretrained_lm_model</span><span class="p">(</span>
    <span class="n">pretrained_model_name</span><span class="o">=</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">collections</span><span class="o">.</span><span class="n">nlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">tokenizers</span><span class="o">.</span><span class="n">get_tokenizer</span><span class="p">(</span>
    <span class="n">tokenizer_name</span><span class="o">=</span><span class="s2">&quot;nemobert&quot;</span><span class="p">,</span>
    <span class="n">pretrained_model_name</span><span class="o">=</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>See examples/nlp/token_classification/token_classification.py on how to use a BERT model that you pre-trained yourself.
Now, create the train and evaluation data layers:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_data_layer</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">data_layers</span><span class="o">.</span><span class="n">BertTokenClassificationDataLayer</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">text_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s1">&#39;text_train.txt&#39;</span><span class="p">),</span>
    <span class="n">label_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s1">&#39;labels_train.txt&#39;</span><span class="p">),</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="n">MAX_SEQ_LENGTH</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>

<span class="n">label_ids</span> <span class="o">=</span> <span class="n">train_data_layer</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">label_ids</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_ids</span><span class="p">)</span>

<span class="n">eval_data_layer</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">data_layers</span><span class="o">.</span><span class="n">BertTokenClassificationDataLayer</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">text_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s1">&#39;text_dev.txt&#39;</span><span class="p">),</span>
    <span class="n">label_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s1">&#39;labels_dev.txt&#39;</span><span class="p">),</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="n">MAX_SEQ_LENGTH</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">label_ids</span><span class="o">=</span><span class="n">label_ids</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>We need to create the classifier to sit on top of the pretrained model and define the loss function:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">bert_model</span><span class="o">.</span><span class="n">hidden_size</span>
<span class="n">ner_classifier</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">collections</span><span class="o">.</span><span class="n">nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">trainables</span><span class="o">.</span><span class="n">TokenClassifier</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                      <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span>
                                      <span class="n">dropout</span><span class="o">=</span><span class="n">CLASSIFICATION_DROPOUT</span><span class="p">)</span>

<span class="n">ner_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLossNM</span><span class="p">(</span><span class="n">logits_ndim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Now, create the train and evaluation datasets:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_type_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">loss_mask</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">train_data_layer</span><span class="p">()</span>

<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                       <span class="n">token_type_ids</span><span class="o">=</span><span class="n">input_type_ids</span><span class="p">,</span>
                       <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">ner_classifier</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">ner_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">loss_mask</span><span class="o">=</span><span class="n">loss_mask</span><span class="p">)</span>


<span class="n">eval_input_ids</span><span class="p">,</span> <span class="n">eval_input_type_ids</span><span class="p">,</span> <span class="n">eval_input_mask</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">eval_subtokens_mask</span><span class="p">,</span> <span class="n">eval_labels</span> \
<span class="o">=</span> <span class="n">eval_data_layer</span><span class="p">()</span>

<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">eval_input_ids</span><span class="p">,</span>
    <span class="n">token_type_ids</span><span class="o">=</span><span class="n">eval_input_type_ids</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">eval_input_mask</span><span class="p">)</span>

<span class="n">eval_logits</span> <span class="o">=</span> <span class="n">ner_classifier</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Now, we will set up our callbacks. We will use 3 callbacks:</p>
<ul>
<li><p><cite>SimpleLossLoggerCallback</cite> to print loss values during training</p></li>
<li><p><cite>EvaluatorCallback</cite> to evaluate our F1 score on the dev dataset. In this example, <cite>EvaluatorCallback</cite> will also output predictions to <cite>output.txt</cite>, which can be helpful with debugging what our model gets wrong.</p></li>
<li><p><cite>CheckpointCallback</cite> to save and restore checkpoints.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">callback_train</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">SimpleLossLoggerCallback</span><span class="p">(</span>
    <span class="n">tensors</span><span class="o">=</span><span class="p">[</span><span class="n">loss</span><span class="p">],</span>
    <span class="n">print_func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loss: {:.3f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())))</span>

<span class="n">train_data_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data_layer</span><span class="p">)</span>

<span class="c1"># If you&#39;re training on multiple GPUs, this should be</span>
<span class="c1"># train_data_size / (batch_size * batches_per_step * num_gpus)</span>
<span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_data_size</span> <span class="o">/</span> <span class="p">(</span><span class="n">BATCHES_PER_STEP</span> <span class="o">*</span> <span class="n">BATCH_SIZE</span><span class="p">))</span>

<span class="n">callback_eval</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">EvaluatorCallback</span><span class="p">(</span>
    <span class="n">eval_tensors</span><span class="o">=</span><span class="p">[</span><span class="n">eval_logits</span><span class="p">,</span> <span class="n">eval_labels</span><span class="p">,</span> <span class="n">eval_subtokens_mask</span><span class="p">],</span>
    <span class="n">user_iter_callback</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">eval_iter_callback</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
    <span class="n">user_epochs_done_callback</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">eval_epochs_done_callback</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">label_ids</span><span class="p">),</span>
    <span class="n">eval_step</span><span class="o">=</span><span class="n">steps_per_epoch</span><span class="p">)</span>

<span class="c1"># Callback to store checkpoints</span>
<span class="c1"># Checkpoints will be stored in checkpoints folder inside WORK_DIR</span>
<span class="n">ckpt_callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">CheckpointCallback</span><span class="p">(</span>
    <span class="n">folder</span><span class="o">=</span><span class="n">nf</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span>
    <span class="n">epoch_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<p>Finally, we will define our learning rate policy and our optimizer, and start training.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr_policy</span> <span class="o">=</span> <span class="n">WarmupAnnealing</span><span class="p">(</span><span class="n">NUM_EPOCHS</span> <span class="o">*</span> <span class="n">steps_per_epoch</span><span class="p">,</span>
                    <span class="n">warmup_ratio</span><span class="o">=</span><span class="n">LR_WARMUP_PROPORTION</span><span class="p">)</span>

<span class="n">nf</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">tensors_to_optimize</span><span class="o">=</span><span class="p">[</span><span class="n">train_loss</span><span class="p">],</span>
         <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">train_callback</span><span class="p">,</span> <span class="n">eval_callback</span><span class="p">,</span> <span class="n">ckpt_callback</span><span class="p">],</span>
         <span class="n">lr_policy</span><span class="o">=</span><span class="n">lr_policy</span><span class="p">,</span>
         <span class="n">optimizer</span><span class="o">=</span><span class="n">OPTIMIZER</span><span class="p">,</span>
         <span class="n">optimization_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="n">NUM_EPOCHS</span><span class="p">,</span>
                              <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">LEARNING_RATE</span><span class="p">})</span>
</pre></div>
</div>
</div></blockquote>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><a class="reference external" href="https://www.tensorflow.org/tensorboard">Tensorboard</a> is a great debugging tool. It’s not a requirement for this tutorial, but if you’d like to use it, you should install <a class="reference external" href="https://github.com/lanpa/tensorboardX">tensorboardX</a> and run the following command during fine-tuning:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tensorboard --logdir output_ner/tensorboard
</pre></div>
</div>
</div>
</div>
<div class="section" id="training-and-inference-scripts">
<span id="ner-scripts"></span><h2>Training and inference scripts<a class="headerlink" href="#training-and-inference-scripts" title="Permalink to this headline">¶</a></h2>
<p>To run the provided training script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python examples/nlp/token_classification/token_classification.py --data_dir path_to_data --work_dir path_to_output_dir
</pre></div>
</div>
<p>To run inference:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python examples/nlp/token_classification/token_classification_infer.py --labels_dict path_to_data/label_ids.csv
--checkpoint_dir path_to_output_dir/checkpoints/
</pre></div>
</div>
<p>Note, label_ids.csv file will be generated during training and stored in the data_dir folder.</p>
</div>
<div class="section" id="bioner">
<h2>BioNER<a class="headerlink" href="#bioner" title="Permalink to this headline">¶</a></h2>
<p>To use BioBERT/BioMegatron for biomedical named entity recognition please visit:</p>
<p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/master/examples/nlp/biobert_notebooks/biobert_ner.ipynb">https://github.com/NVIDIA/NeMo/blob/master/examples/nlp/biobert_notebooks/biobert_ner.ipynb</a></p>
</div>
<div class="section" id="using-other-bert-models">
<h2>Using Other BERT Models<a class="headerlink" href="#using-other-bert-models" title="Permalink to this headline">¶</a></h2>
<p>In addition to using pre-trained BERT models from Google and BERT models that you’ve trained yourself, in NeMo it’s possible to use other third-party BERT models as well, as long as the weights were exported with PyTorch. For example, if you want to fine-tune an NER task with <a class="reference external" href="https://github.com/allenai/scibert">SciBERT</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar
tar -xf scibert_scivocab_cased.tar
<span class="nb">cd</span> scibert_scivocab_cased
tar -xzf weights.tar.gz
mv bert_config.json config.json
<span class="nb">cd</span> ..
</pre></div>
</div>
<p>And then, when you load your BERT model, you should specify the name of the directory for the model name.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">NemoBertTokenizer</span><span class="p">(</span><span class="n">pretrained_model</span><span class="o">=</span><span class="s2">&quot;scibert_scivocab_cased&quot;</span><span class="p">)</span>
<span class="n">bert_model</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">trainables</span><span class="o">.</span><span class="n">huggingface</span><span class="o">.</span><span class="n">BERT</span><span class="p">(</span>
    <span class="n">pretrained_model_name</span><span class="o">=</span><span class="s2">&quot;scibert_scivocab_cased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="punctuation.html" class="btn btn-neutral float-right" title="Tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="text_classification.html" class="btn btn-neutral float-left" title="Tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2020, NVIDIA

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>