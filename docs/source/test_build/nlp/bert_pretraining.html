

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>BERT Pre-training Tutorial &mdash; nemo 0.11.0b9 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Transformer Language Model Tutorial" href="transformer_language_model.html" />
    <link rel="prev" title="Tutorial" href="neural_machine_translation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nemo
          

          
          </a>

          
            
            
              <div class="version">
                0.11.0b9
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Fast Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asr/intro.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speaker_recognition/intro.html">Speaker Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech_command/intro.html">Speech Commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../voice_activity_detection/intro.html">Voice Activity Detection</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="intro.html">Natural Language Processing</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html#neural-machine-translation-nmt">Neural Machine Translation (NMT)</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="intro.html#pretraining-bert">Pretraining BERT</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">BERT Pre-training Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#download-pretrained-models">Download pretrained models</a></li>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#download-corpus">Download Corpus</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-the-tokenizer">Create the tokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-the-model">Create the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-the-model">Run the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-to-use-the-training-script">How to use the training script</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#transformer-language-model">Transformer Language Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#megatron-lm-for-downstream-tasks">Megatron-LM for Downstream tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#glue-benchmark">GLUE Benchmark</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#intent-and-slot-filling">Intent and Slot filling</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#text-classification">Text Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#named-entity-recognition">Named Entity Recognition</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#punctuation-and-word-capitalization">Punctuation and Word Capitalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#question-answering">Question Answering</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#dialogue-state-tracking">Dialogue State Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#asr-postprocessing-with-bert">ASR Postprocessing with BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tts/intro.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../collections/modules.html">NeMo Collections API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-docs/modules.html">NeMo API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chinese/intro.html">中文支持</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nemo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="intro.html">Natural Language Processing</a> &raquo;</li>
        
      <li>BERT Pre-training Tutorial</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/nlp/bert_pretraining.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="bert-pre-training-tutorial">
<span id="bert-pretraining"></span><h1>BERT Pre-training Tutorial<a class="headerlink" href="#bert-pre-training-tutorial" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we will build and train a masked language model, either from scratch or from a pretrained BERT model, using the BERT architecture <a class="bibtex reference internal" href="#nlp-bert-devlin2018bert" id="id1">[NLP-BERT-PRETRAINING2]</a>.
Make sure you have <code class="docutils literal notranslate"><span class="pre">nemo</span></code> and <code class="docutils literal notranslate"><span class="pre">nemo_nlp</span></code> installed before starting this tutorial. See the <a class="reference internal" href="../index.html#installation"><span class="std std-ref">Getting started</span></a> section for more details.</p>
<p>The code used in this tutorial can be found at <code class="docutils literal notranslate"><span class="pre">examples/nlp/language_modeling/bert_pretraining.py</span></code>.</p>
<div class="section" id="download-pretrained-models">
<span id="pretrained-models-bert"></span><h2>Download pretrained models<a class="headerlink" href="#download-pretrained-models" title="Permalink to this headline">¶</a></h2>
<p>Pretrained BERT models and model configuration files can be downloaded at following links.</p>
<p>BERT Large models (~330M parameters):
<a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:bertlargeuncasedfornemo">https://ngc.nvidia.com/catalog/models/nvidia:bertlargeuncasedfornemo</a></p>
<p>BERT Base models (~110M parameters):
<a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:bertbaseuncasedfornemo">https://ngc.nvidia.com/catalog/models/nvidia:bertbaseuncasedfornemo</a>
<a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:bertbasecasedfornemo">https://ngc.nvidia.com/catalog/models/nvidia:bertbasecasedfornemo</a></p>
<p>Model results on downstream tasks:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 48%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" rowspan="2"><p>Model</p></th>
<th class="head" colspan="2"><p>SQuADv1.1</p></th>
<th class="head" colspan="2"><p>SQuADv2.0</p></th>
<th class="head" colspan="2"><p>GLUE MRPC</p></th>
</tr>
<tr class="row-even"><th class="head"><p>EM</p></th>
<th class="head"><p>F1</p></th>
<th class="head"><p>EM</p></th>
<th class="head"><p>F1</p></th>
<th class="head"><p>Acc</p></th>
<th class="head"><p>F1</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p>BERT-base-uncased</p></td>
<td><p>82.74%</p></td>
<td><p>89.79%</p></td>
<td><p>71.24%</p></td>
<td><p>74.32%</p></td>
<td><p>86.52%</p></td>
<td><p>90.53%</p></td>
</tr>
<tr class="row-even"><td><p>BERT-large-uncased</p></td>
<td><p>85.79%</p></td>
<td><p>92.28%</p></td>
<td><p>80.17%</p></td>
<td><p>83.32%</p></td>
<td><p>88.72%</p></td>
<td><p>91.96%</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Creating domain-specific BERT models can be advantageous for a wide range of applications. One notable is domain-specific BERT in a biomedical setting,
similar to BioBERT <a class="bibtex reference internal" href="#nlp-bert-lee2019biobert" id="id2">[NLP-BERT-PRETRAINING3]</a> and SciBERT <a class="bibtex reference internal" href="#nlp-bert-beltagy2019scibert" id="id3">[NLP-BERT-PRETRAINING1]</a>.</p>
</div>
<div class="section" id="download-corpus">
<span id="bert-data-download"></span><h2>Download Corpus<a class="headerlink" href="#download-corpus" title="Permalink to this headline">¶</a></h2>
<p>The training corpus can be either raw text where data preprocessing is done on the fly or an already preprocessed data set. In the following we will give examples for both.
To showcase how to train on raw text data, we will be using the very small WikiText-2 dataset <a class="bibtex reference internal" href="#nlp-bert-merity2016pointer" id="id4">[NLP-BERT-PRETRAINING4]</a>.</p>
<p>To download the dataset, run the script <code class="docutils literal notranslate"><span class="pre">examples/nlp/language_modeling/get_wkt2.sh</span> <span class="pre">download_dir</span></code>. After downloading and unzipping, the folder is located at <cite>download_dir</cite> and should include 3 files that look like this:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>test.txt
train.txt
valid.txt
</pre></div>
</div>
</div></blockquote>
<p>To train BERT on a Chinese dataset, you may download the Chinese Wikipedia corpus <a class="reference external" href="https://github.com/brightmart/nlp_chinese_corpus">wiki2019zh</a>. After downloading, you may unzip and
use the script <code class="docutils literal notranslate"><span class="pre">examples/nlp/language_modeling/process_wiki_zh.py</span></code> for preprocessing the raw text.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python examples/nlp/language_modeling/process_wiki_zh.py --data_dir<span class="o">=</span>./wiki_zh --output_dir<span class="o">=</span>./wiki_zh --min_frequency<span class="o">=</span><span class="m">3</span>
</pre></div>
</div>
</div></blockquote>
<p>For already preprocessed data, we will be using a large dataset composed of Wikipedia and BookCorpus as in the original BERT paper.</p>
<p>To download the dataset, go to <a class="reference external" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT#quick-start-guide">https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT#quick-start-guide</a>,
follow steps 1-5 in the Quick-Start-Guide and run the script <code class="docutils literal notranslate"><span class="pre">./data/create_datasets_from_start.sh</span></code> inside the docker container.
The downloaded folder should include a 2 sub folders with the prefix <cite>lower_case_[0,1]_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5</cite>
and <cite>lower_case_[0,1]_seq_len_512_max_pred_80_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5</cite>, containing sequences of length 128 with a maximum of 20 masked tokens
and sequences of length 512 with a maximum of 80 masked tokens respectively.</p>
</div>
<div class="section" id="create-the-tokenizer">
<h2>Create the tokenizer<a class="headerlink" href="#create-the-tokenizer" title="Permalink to this headline">¶</a></h2>
<p>A tokenizer will be used for data preprocessing and, therefore, is only required for training using raw text data.</p>
<p><cite>BERTPretrainingDataDesc</cite> converts your dataset into the format compatible with <cite>BertPretrainingDataset</cite>. The most computationally intensive step is to tokenize
the dataset to create a vocab file and a tokenizer model.</p>
<p>You can also use an available vocab or tokenizer model to skip this step. If you already have a pretrained tokenizer model
copy it to the <cite>[data_dir]/bert</cite> folder under the name <cite>tokenizer.model</cite> and the script will skip this step.</p>
<p>If have an available vocab, such as <cite>vocab.txt</cite> file from any pretrained BERT model, copy it to the <cite>[data_dir]/bert</cite> folder under the name <cite>vocab.txt</cite>.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nemo.collections.nlp</span> <span class="kn">as</span> <span class="nn">nemo_nlp</span>

<span class="n">data_desc</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">BERTPretrainingDataDesc</span><span class="p">(</span>
                <span class="n">dataset_name</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span>
                <span class="n">train_data</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">train_data</span><span class="p">,</span>
                <span class="n">eval_data</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">eval_data</span><span class="p">,</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">sample_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">sample_size</span><span class="p">,</span>
                <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>We need to define our tokenizer. If you’d like to use a custom vocabulary file, we strongly recommend you use our <cite>SentencePieceTokenizer</cite>.
Otherwise, if you’ll be using a vocabulary file from another pre-trained BERT model, you should use <cite>NemoBertTokenizer</cite>.</p>
<p>To train on a Chinese dataset, you should use <cite>NemoBertTokenizer</cite>.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If you&#39;re using a custom vocabulary, create your tokenizer like this</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">SentencePieceTokenizer</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;tokenizer.model&quot;</span><span class="p">)</span>
<span class="n">special_tokens</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">get_bert_special_tokens</span><span class="p">(</span><span class="s1">&#39;bert&#39;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens</span><span class="p">)</span>

<span class="c1"># Otherwise, create your tokenizer like this</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">NemoBertTokenizer</span><span class="p">(</span><span class="n">pretrained_model</span><span class="o">=</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="c1"># or</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">NemoBertTokenizer</span><span class="p">(</span><span class="n">vocab_file</span><span class="o">=</span><span class="s2">&quot;vocab.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="create-the-model">
<h2>Create the model<a class="headerlink" href="#create-the-model" title="Permalink to this headline">¶</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We recommend you try this out in a Jupyter notebook. It’ll make debugging much easier!</p>
</div>
<p>First, we need to create our neural factory with the supported backend. How you should define it depends on whether you’d like to multi-GPU or mixed-precision training.
This tutorial assumes that you’re training on one GPU, without mixed precision. If you want to use mixed precision, set <code class="docutils literal notranslate"><span class="pre">amp_opt_level</span></code> to <code class="docutils literal notranslate"><span class="pre">O1</span></code> or <code class="docutils literal notranslate"><span class="pre">O2</span></code>.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nf</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">NeuralModuleFactory</span><span class="p">(</span><span class="n">local_rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">,</span>
                                   <span class="n">optimization_level</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">amp_opt_level</span><span class="p">,</span>
                                   <span class="n">log_dir</span><span class="o">=</span><span class="n">work_dir</span><span class="p">,</span>
                                   <span class="n">create_tb_writer</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                   <span class="n">files_to_copy</span><span class="o">=</span><span class="p">[</span><span class="vm">__file__</span><span class="p">])</span>
</pre></div>
</div>
</div></blockquote>
<p>We also need to define the BERT model that we will be pre-training. Here, you can configure your model size as needed. If you want to train from scratch, use this:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bert_model</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">trainables</span><span class="o">.</span><span class="n">huggingface</span><span class="o">.</span><span class="n">BERT</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">num_hidden_layers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
    <span class="n">intermediate_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
    <span class="n">max_position_embeddings</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">,</span>
    <span class="n">hidden_act</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to start pre-training from existing BERT checkpoints, specify the checkpoint folder path with the argument <code class="docutils literal notranslate"><span class="pre">--load_dir</span></code>.</p>
</div>
<p>The following code will automatically load the checkpoints if they exist and are compatible to the previously defined model</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ckpt_callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">CheckpointCallback</span><span class="p">(</span><span class="n">folder</span><span class="o">=</span><span class="n">nf</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span>
                    <span class="n">load_from_folder</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">load_dir</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>To initialize the model with already pretrained checkpoints, specify <code class="docutils literal notranslate"><span class="pre">pretrained_model_name</span></code>. For example, to initialize BERT Base trained on cased Wikipedia and BookCorpus with 12 layers, run</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bert_model</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">trainables</span><span class="o">.</span><span class="n">huggingface</span><span class="o">.</span><span class="n">BERT</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="o">=</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>For the full list of BERT model names, check out <cite>nemo_nlp.nm.trainables.huggingface.BERT.list_pretrained_models()</cite>.</p>
<p>Next, we will define our classifier and loss functions. We will demonstrate how to pre-train with both MLM (masked language model) and NSP (next sentence prediction) losses,
but you may observe higher downstream accuracy by only pre-training with MLM loss.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mlm_classifier</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">trainables</span><span class="o">.</span><span class="n">BertTokenClassifier</span><span class="p">(</span>
                            <span class="n">args</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">num_classes</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
                            <span class="n">activation</span><span class="o">=</span><span class="n">ACT2FN</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">],</span>
                            <span class="n">log_softmax</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">mlm_loss_fn</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SmoothedCrossEntropyLoss</span><span class="p">()</span>

<span class="n">nsp_classifier</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">trainables</span><span class="o">.</span><span class="n">SequenceClassifier</span><span class="p">(</span>
                                        <span class="n">args</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
                                        <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                        <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                        <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
                                        <span class="n">log_softmax</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">nsp_loss_fn</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">CrossEntropyLossNM</span><span class="p">()</span>

<span class="n">bert_loss</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">LossAggregatorNM</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Finally we will tie the weights of the encoder embedding layer and the MLM output embedding:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mlm_classifier</span><span class="o">.</span><span class="n">tie_weights_with</span><span class="p">(</span>
    <span class="n">bert_model</span><span class="p">,</span>
    <span class="n">weight_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mlp.last_linear_layer.weight&quot;</span><span class="p">],</span>
    <span class="n">name2name_and_transform</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;mlp.last_linear_layer.weight&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;bert.embeddings.word_embeddings.weight&quot;</span><span class="p">,</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">WeightShareTransform</span><span class="o">.</span><span class="n">SAME</span><span class="p">)</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Then, we create the pipeline from input to output that can be used for both training and evaluation:</p>
<p>For training from raw text use <cite>nemo_nlp.nm.data_layers.BertPretrainingDataLayer</cite>, for preprocessed data use <cite>nemo_nlp.nm.data_layers.BertPretrainingPreprocessedDataLayer</cite></p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_pipeline</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">):</span>
    <span class="n">data_layer</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">data_layers</span><span class="o">.</span><span class="n">BertPretrainingDataLayer</span><span class="p">(</span>
                            <span class="n">tokenizer</span><span class="p">,</span>
                            <span class="n">data_file</span><span class="p">,</span>
                            <span class="n">max_seq_length</span><span class="p">,</span>
                            <span class="n">mask_probability</span><span class="p">,</span>
                            <span class="n">short_seq_prob</span><span class="p">,</span>
                            <span class="n">batch_size</span><span class="p">)</span>
    <span class="c1"># for preprocessed data</span>
    <span class="c1"># data_layer = nemo_nlp.BertPretrainingPreprocessedDataLayer(</span>
    <span class="c1">#        data_file,</span>
    <span class="c1">#        max_predictions_per_seq,</span>
    <span class="c1">#        batch_size,</span>
    <span class="c1">#        mode)</span>

    <span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_layer</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">batches_per_step</span><span class="p">)</span>

    <span class="n">input_data</span> <span class="o">=</span> <span class="n">data_layer</span><span class="p">()</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_data</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
                               <span class="n">token_type_ids</span><span class="o">=</span><span class="n">input_data</span><span class="o">.</span><span class="n">input_type_ids</span><span class="p">,</span>
                               <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_data</span><span class="o">.</span><span class="n">input_mask</span><span class="p">)</span>

    <span class="n">mlm_logits</span> <span class="o">=</span> <span class="n">mlm_classifier</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">mlm_loss</span> <span class="o">=</span> <span class="n">mlm_loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">mlm_logits</span><span class="p">,</span>
                           <span class="n">labels</span><span class="o">=</span><span class="n">input_data</span><span class="o">.</span><span class="n">output_ids</span><span class="p">,</span>
                           <span class="n">output_mask</span><span class="o">=</span><span class="n">input_data</span><span class="o">.</span><span class="n">output_mask</span><span class="p">)</span>

    <span class="n">nsp_logits</span> <span class="o">=</span> <span class="n">nsp_classifier</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">nsp_loss</span> <span class="o">=</span> <span class="n">nsp_loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">nsp_logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_data</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">bert_loss</span><span class="p">(</span><span class="n">loss_1</span><span class="o">=</span><span class="n">mlm_loss</span><span class="p">,</span> <span class="n">loss_2</span><span class="o">=</span><span class="n">nsp_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">mlm_loss</span><span class="p">,</span> <span class="n">nsp_loss</span><span class="p">,</span> <span class="n">steps_per_epoch</span>


<span class="n">train_loss</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="n">create_pipeline</span><span class="p">(</span>
                            <span class="n">data_file</span><span class="o">=</span><span class="n">data_desc</span><span class="o">.</span><span class="n">train_file</span><span class="p">,</span>
                            <span class="n">preprocessed_data</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                            <span class="n">max_seq_length</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">,</span>
                            <span class="n">mask_probability</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">mask_probability</span><span class="p">,</span>
                            <span class="n">short_seq_prob</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">short_seq_prob</span><span class="p">,</span>
                            <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                            <span class="n">batches_per_step</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batches_per_step</span><span class="p">,</span>
                            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

<span class="c1"># for preprocessed data</span>
<span class="c1"># train_loss, _, _, steps_per_epoch = create_pipeline(</span>
<span class="c1">#                            data_file=args.train_data,</span>
<span class="c1">#                            preprocessed_data=True,</span>
<span class="c1">#                            max_predictions_per_seq=args.max_predictions_per_seq,</span>
<span class="c1">#                            batch_size=args.batch_size,</span>
<span class="c1">#                            batches_per_step=args.batches_per_step,</span>
<span class="c1">#                            mode=&quot;train&quot;)</span>

<span class="n">eval_loss</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">create_pipeline</span><span class="p">(</span>
                                <span class="n">data_file</span><span class="o">=</span><span class="n">data_desc</span><span class="o">.</span><span class="n">eval_file</span><span class="p">,</span>
                                <span class="n">preprocessed_data</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                <span class="n">max_seq_length</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">,</span>
                                <span class="n">mask_probability</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">mask_probability</span><span class="p">,</span>
                                <span class="n">short_seq_prob</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">short_seq_prob</span><span class="p">,</span>
                                <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                                <span class="n">batches_per_step</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batches_per_step</span><span class="p">,</span>
                                <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;eval&quot;</span><span class="p">)</span>

<span class="c1"># for preprocessed data</span>
<span class="c1"># eval_loss, eval_mlm_loss, eval_nsp_loss, _ = create_pipeline(</span>
<span class="c1">#                            data_file=args.eval_data,</span>
<span class="c1">#                            preprocessed_data=True,</span>
<span class="c1">#                            max_predictions_per_seq=args.max_predictions_per_seq,</span>
<span class="c1">#                            batch_size=args.batch_size,</span>
<span class="c1">#                            batches_per_step=args.batches_per_step,</span>
<span class="c1">#                            mode=&quot;eval&quot;)</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="run-the-model">
<h2>Run the model<a class="headerlink" href="#run-the-model" title="Permalink to this headline">¶</a></h2>
<p>Define your learning rate policy</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr_policy_fn</span> <span class="o">=</span> <span class="n">get_lr_policy</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">lr_policy</span><span class="p">,</span>
                            <span class="n">total_steps</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_iters</span><span class="p">,</span>
                            <span class="n">warmup_ratio</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_warmup_proportion</span><span class="p">)</span>

<span class="c1"># if you are training on raw text data, you have use the alternative to set the number of training epochs</span>
<span class="n">lr_policy_fn</span> <span class="o">=</span> <span class="n">get_lr_policy</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">lr_policy</span><span class="p">,</span>
                             <span class="n">total_steps</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">*</span> <span class="n">steps_per_epoch</span><span class="p">,</span>
                             <span class="n">warmup_ratio</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_warmup_proportion</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Next, we define necessary callbacks:</p>
<ol class="arabic">
<li><p><cite>SimpleLossLoggerCallback</cite>: tracking loss during training</p></li>
<li><p><cite>EvaluatorCallback</cite>: tracking metrics during evaluation at set intervals</p></li>
<li><p><cite>CheckpointCallback</cite>: saving model checkpoints at set intervals</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">SimpleLossLoggerCallback</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="p">[</span><span class="n">train_loss</span><span class="p">],</span>
    <span class="n">print_func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loss: {:.3f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())))),</span>
    <span class="n">step_freq</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">train_step_freq</span><span class="p">,</span>
<span class="n">eval_callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">EvaluatorCallback</span><span class="p">(</span><span class="n">eval_tensors</span><span class="o">=</span><span class="p">[</span><span class="n">eval_loss</span><span class="p">],</span>
    <span class="n">user_iter_callback</span><span class="o">=</span><span class="n">nemo_nlp</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">lm_bert_callback</span><span class="o">.</span><span class="n">eval_iter_callback</span><span class="p">,</span>
    <span class="n">user_epochs_done_callback</span><span class="o">=</span><span class="n">nemo_nlp</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">lm_bert_callback</span><span class="o">.</span><span class="n">eval_epochs_done_callback</span>
    <span class="n">eval_step</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">eval_step_freq</span><span class="p">)</span>
<span class="n">ckpt_callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">CheckpointCallback</span><span class="p">(</span><span class="n">folder</span><span class="o">=</span><span class="n">nf</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span>
    <span class="n">epoch_freq</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">save_epoch_freq</span><span class="p">,</span>
    <span class="n">load_from_folder</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">load_dir</span><span class="p">,</span>
    <span class="n">step_freq</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">save_step_freq</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
<p>We recommend you export your model’s parameters to a config file. This makes it easier to load your BERT model into NeMo later, as explained in our Named Entity Recognition <a class="reference internal" href="ner.html#ner-tutorial"><span class="std std-ref">Introduction</span></a> tutorial.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config_path</span> <span class="o">=</span> <span class="n">f</span><span class="s1">&#39;{nf.checkpoint_dir}/bert-config.json&#39;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">config_path</span><span class="p">):</span>
    <span class="n">bert_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">to_json_file</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Finally, you should define your optimizer, and start training!</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nf</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">tensors_to_optimize</span><span class="o">=</span><span class="p">[</span><span class="n">train_loss</span><span class="p">],</span>
         <span class="n">lr_policy</span><span class="o">=</span><span class="n">lr_policy_fn</span><span class="p">,</span>
         <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">train_callback</span><span class="p">,</span> <span class="n">eval_callback</span><span class="p">,</span> <span class="n">ckpt_callback</span><span class="p">],</span>
         <span class="n">optimizer</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span>
         <span class="n">optimization_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                              <span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">,</span>
                              <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
                              <span class="s2">&quot;betas&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">beta2</span><span class="p">),</span>
                              <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">})</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="how-to-use-the-training-script">
<h2>How to use the training script<a class="headerlink" href="#how-to-use-the-training-script" title="Permalink to this headline">¶</a></h2>
<p>You can find the example training script at <code class="docutils literal notranslate"><span class="pre">examples/nlp/language_modeling/bert_pretraining.py</span></code>.</p>
<p>For single GPU training, the script can be started with</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/nlp/language_modeling
python bert_pretraining.py --config_file bert-config.json <span class="o">[</span>args<span class="o">]</span>
</pre></div>
</div>
<p>The BERT configuration files can be found in the NGC model repositories, see <a class="reference internal" href="#pretrained-models-bert"><span class="std std-ref">Download pretrained models</span></a>.</p>
<p>For multi-GPU training with <code class="docutils literal notranslate"><span class="pre">x</span></code> GPUs, the script can be started with</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/nlp/language_modeling
python -m torch.distributed.launch --nproc_per_node<span class="o">=</span>x bert_pretraining.py --num_gpus<span class="o">=</span>x <span class="o">[</span>args<span class="o">]</span>
</pre></div>
</div>
<p>If you running the model on raw text data, please remember to add the argument <code class="docutils literal notranslate"><span class="pre">data_text</span></code> to the python command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python bert_pretraining.py <span class="o">[</span>args<span class="o">]</span> data_text <span class="o">[</span>args<span class="o">]</span>
</pre></div>
</div>
<p>Similarly, to run the model on already preprocessed data add the argument <code class="docutils literal notranslate"><span class="pre">data_preprocessed</span></code> to the python command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python bert_pretraining.py <span class="o">[</span>args<span class="o">]</span> data_preprocessed <span class="o">[</span>args<span class="o">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, the script assumes <code class="docutils literal notranslate"><span class="pre">data_preprocessed</span></code> as input mode.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For downloading or preprocessing data offline please refer to <a class="reference internal" href="#bert-data-download"><span class="std std-ref">Download Corpus</span></a>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><a class="reference external" href="https://www.tensorflow.org/tensorboard">Tensorboard</a> is a great debugging tool. It’s not a requirement for this tutorial, but if you’d like to use it, you should install <a class="reference external" href="https://github.com/lanpa/tensorboardX">tensorboardX</a> and run the following command during pre-training:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tensorboard --logdir outputs/bert_lm/tensorboard
</pre></div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-nlp/bert_pretraining-0"><dl class="citation">
<dt class="bibtex label" id="nlp-bert-beltagy2019scibert"><span class="brackets"><a class="fn-backref" href="#id3">NLP-BERT-PRETRAINING1</a></span></dt>
<dd><p>Iz Beltagy, Arman Cohan, and Kyle Lo. Scibert: pretrained contextualized embeddings for scientific text. 2019. <a class="reference external" href="https://arxiv.org/abs/1903.10676">arXiv:1903.10676</a>.</p>
</dd>
<dt class="bibtex label" id="nlp-bert-devlin2018bert"><span class="brackets"><a class="fn-backref" href="#id1">NLP-BERT-PRETRAINING2</a></span></dt>
<dd><p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>, 2018.</p>
</dd>
<dt class="bibtex label" id="nlp-bert-lee2019biobert"><span class="brackets"><a class="fn-backref" href="#id2">NLP-BERT-PRETRAINING3</a></span></dt>
<dd><p>Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. 2019. <a class="reference external" href="https://arxiv.org/abs/1901.08746">arXiv:1901.08746</a>.</p>
</dd>
<dt class="bibtex label" id="nlp-bert-merity2016pointer"><span class="brackets"><a class="fn-backref" href="#id4">NLP-BERT-PRETRAINING4</a></span></dt>
<dd><p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. <em>arXiv preprint arXiv:1609.07843</em>, 2016.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="transformer_language_model.html" class="btn btn-neutral float-right" title="Transformer Language Model Tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="neural_machine_translation.html" class="btn btn-neutral float-left" title="Tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2020, NVIDIA

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>