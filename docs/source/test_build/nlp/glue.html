

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial &mdash; nemo 0.11.0b9 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorial" href="joint_intent_slot_filling.html" />
    <link rel="prev" title="Megatron-LM for Downstream Tasks" href="megatron_finetuning.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nemo
          

          
          </a>

          
            
            
              <div class="version">
                0.11.0b9
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Fast Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asr/intro.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speaker_recognition/intro.html">Speaker Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech_command/intro.html">Speech Commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../voice_activity_detection/intro.html">Voice Activity Detection</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="intro.html">Natural Language Processing</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html#neural-machine-translation-nmt">Neural Machine Translation (NMT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#pretraining-bert">Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#transformer-language-model">Transformer Language Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#megatron-lm-for-downstream-tasks">Megatron-LM for Downstream tasks</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="intro.html#glue-benchmark">GLUE Benchmark</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#glue-tasks-description">GLUE tasks description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-the-model">Training the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-training">Model training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-results">Model results</a></li>
<li class="toctree-l4"><a class="reference internal" href="#evaluating-checkpoints">Evaluating Checkpoints</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#intent-and-slot-filling">Intent and Slot filling</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#text-classification">Text Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#named-entity-recognition">Named Entity Recognition</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#punctuation-and-word-capitalization">Punctuation and Word Capitalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#question-answering">Question Answering</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#dialogue-state-tracking">Dialogue State Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#asr-postprocessing-with-bert">ASR Postprocessing with BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tts/intro.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../collections/modules.html">NeMo Collections API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-docs/modules.html">NeMo API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chinese/intro.html">中文支持</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nemo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="intro.html">Natural Language Processing</a> &raquo;</li>
        
      <li>Tutorial</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/nlp/glue.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tutorial">
<span id="glue"></span><h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we are going to describe how to finetune a BERT-like model based on <a class="reference external" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <a class="bibtex reference internal" href="#nlp-glue-devlin2018bert" id="id1">[NLP-GLUE2]</a> on <a class="reference external" href="https://openreview.net/pdf?id=rJ4km2R5t7">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a> <a class="bibtex reference internal" href="#nlp-glue-wang2018glue" id="id2">[NLP-GLUE6]</a>.
The code used in this tutorial is based on <code class="docutils literal notranslate"><span class="pre">examples/nlp/glue_benchmark/glue_benchmark_with_bert.py</span></code>.</p>
<div class="section" id="glue-tasks-description">
<h2>GLUE tasks description<a class="headerlink" href="#glue-tasks-description" title="Permalink to this headline">¶</a></h2>
<p>GLUE Benchmark includes 9 natural language understanding tasks:</p>
<p>Single-Sentence Tasks:</p>
<ul class="simple">
<li><p><strong>CoLA</strong> The Corpus of Linguistic Acceptability <a class="bibtex reference internal" href="#nlp-glue-warstadt2018neural" id="id3">[NLP-GLUE7]</a> is a set of English sentences from published linguistics literature. The task is to predict whether a given sentence is grammatically correct or not.</p></li>
<li><p><strong>SST-2</strong> The Stanford Sentiment Treebank <a class="bibtex reference internal" href="#nlp-glue-socher2013recursive" id="id4">[NLP-GLUE5]</a> consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence: positive or negative.</p></li>
</ul>
<p>Similarity and Paraphrase tasks:</p>
<ul class="simple">
<li><p><strong>MRPC</strong> The Microsoft Research Paraphrase Corpus <a class="bibtex reference internal" href="#nlp-glue-dolan-brockett-2005-automatically" id="id5">[NLP-GLUE3]</a> is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.</p></li>
<li><p><strong>QQP</strong> <a class="reference external" href="https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs">The Quora Question Pairs2</a> dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent.</p></li>
<li><p><strong>STS-B</strong> The Semantic Textual Similarity Benchmark <a class="bibtex reference internal" href="#nlp-glue-cer2017semeval" id="id6">[NLP-GLUE1]</a> is a collection of sentence pairs drawn from news headlines, video, and image captions, and natural language inference data. The task is to determine how similar two sentences are.</p></li>
</ul>
<p>Inference Tasks:</p>
<ul class="simple">
<li><p><strong>MNLI</strong> The Multi-Genre Natural Language Inference Corpus <a class="bibtex reference internal" href="#nlp-glue-williams2017broad" id="id7">[NLP-GLUE8]</a> is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral).  The task has the matched (in-domain) and mismatched (cross-domain) sections.</p></li>
<li><p><strong>QNLI</strong> The Stanford Question Answering Dataset (:cite: <cite>nlp-glue-rajpurkar2016squad</cite>) is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question. The task is to determine whether the context sentence contains the answer to the question.</p></li>
<li><p><strong>RTE</strong> The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges. The task is to determine whether the second sentence is the entailment of the first one or not.</p></li>
<li><p><strong>WNLI</strong> The Winograd Schema Challenge <a class="bibtex reference internal" href="#nlp-glue-levesque2012winograd" id="id8">[NLP-GLUE4]</a> is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices.</p></li>
</ul>
<p>All the tasks are classification tasks, except for the STS-B task which is a regression task.
All classification tasks are 2-class tasks, except for the MNLI task which is a 3-class task.</p>
<p>More details about GLUE benchmark could be found <a class="reference external" href="https://gluebenchmark.com/tasks">here</a>.</p>
</div>
<div class="section" id="training-the-model">
<h2>Training the model<a class="headerlink" href="#training-the-model" title="Permalink to this headline">¶</a></h2>
<p>Before running <code class="docutils literal notranslate"><span class="pre">examples/nlp/glue_benchmark/glue_benchmark_with_bert.py</span></code>, download the GLUE data with <a class="reference external" href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e">this script</a> by running:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># download the script to get the GLUE data</span>
wget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py
<span class="c1"># run the script to download the GLUE data</span>
python download_glue_data.py
</pre></div>
</div>
<p>After running the above commands, you will have a folder <code class="docutils literal notranslate"><span class="pre">glue_data</span></code> with data folders for every GLUE task. For example, data for MRPC task would be under <code class="docutils literal notranslate"><span class="pre">glue_data/MRPC</span></code>.</p>
<p>The GLUE tasks can be fine-tuned on 4 pre-trained back-bone models supported in NeMo: Megatron-LM BERT, BERT, AlBERT and RoBERTa.
See the list of available pre-trained Huggingface models <a class="reference external" href="https://huggingface.co/transformers/pretrained_models.html">here</a>.
To get the list of all NeMo supported pre-trained models run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nemo.collections.nlp</span> <span class="kn">as</span> <span class="nn">nemo_nlp</span>
<span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">trainables</span><span class="o">.</span><span class="n">get_pretrained_lm_models_list</span><span class="p">()</span>
</pre></div>
</div>
<p>Specify the model to use for training with <code class="docutils literal notranslate"><span class="pre">--pretrained_model_name</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It’s recommended to finetune the model on each task separately.
Also, based on <a class="reference external" href="https://gluebenchmark.com/faq">GLUE Benchmark FAQ#12</a>,
there are might be some differences in dev/test distributions for QQP task
and in train/dev for WNLI task.</p>
</div>
</div>
<div class="section" id="model-training">
<h2>Model training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h2>
<p>Use <code class="docutils literal notranslate"><span class="pre">--task_name</span></code> argument to run the training script on a specific task, use lower cased task name: <code class="docutils literal notranslate"><span class="pre">cola,</span> <span class="pre">sst-2,</span> <span class="pre">mrpc,</span> <span class="pre">sts-b,</span> <span class="pre">qqp,</span> <span class="pre">mnli,</span> <span class="pre">qnli,</span> <span class="pre">rte,</span> <span class="pre">wnli</span></code>.</p>
<p>To run the script on MRPC task on a single GPU, run:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python glue_benchmark_with_bert.py  <span class="se">\</span>
    --data_dir /path_to_data_dir/MRPC <span class="se">\</span>
    --task_name mrpc <span class="se">\</span>
    --work_dir /path_to_output_folder <span class="se">\</span>
    --pretrained_model_name bert-base-uncased
</pre></div>
</div>
</div></blockquote>
<p>To use multi-gpu training on MNLI task, run:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">NUM_GPUS</span><span class="o">=</span><span class="m">4</span>
python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="nv">$NUM_GPUS</span> glue_benchmark_with_bert.py <span class="se">\</span>
    --data_dir<span class="o">=</span>/path_to_data_dir/MNLI <span class="se">\</span>
    --task_name mnli <span class="se">\</span>
    --work_dir /path_to_output_folder <span class="se">\</span>
    --num_gpus<span class="o">=</span><span class="nv">$NUM_GPUS</span> <span class="se">\</span>
    --pretrained_model_name bert-base-uncased <span class="se">\</span>
</pre></div>
</div>
</div></blockquote>
<p>More details about multi-gpu training could be found in the <a class="reference external" href="https://nvidia.github.io/NeMo/training.html">Fast Training</a> section.</p>
<p>For additional model training parameters, please see <code class="docutils literal notranslate"><span class="pre">examples/nlp/glue_benchmark_with_bert.py</span></code>.</p>
</div>
<div class="section" id="model-results">
<h2>Model results<a class="headerlink" href="#model-results" title="Permalink to this headline">¶</a></h2>
<p>Results after finetuning on the specific task (average result after 3 runs) using different pre-trained models:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># to reproduce BERT base paper results</span>
<span class="o">--</span><span class="n">pretrained_model_name</span> <span class="n">bert</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">uncased</span>

<span class="c1"># Albert-large</span>
<span class="o">--</span><span class="n">pretrained_model_name</span> <span class="n">albert</span><span class="o">-</span><span class="n">large</span><span class="o">-</span><span class="n">v2</span>

<span class="c1"># Albert-xlarge</span>
<span class="o">--</span><span class="n">pretrained_model_name</span> <span class="n">albert</span><span class="o">-</span><span class="n">xlarge</span><span class="o">-</span><span class="n">v2</span>

<span class="c1"># Megatron</span>
<span class="o">--</span><span class="n">pretrained_model_name</span> <span class="n">megatron</span><span class="o">-</span><span class="n">bert</span><span class="o">-</span><span class="mi">345</span><span class="n">m</span><span class="o">-</span><span class="n">uncased</span>
</pre></div>
</div>
</div></blockquote>
<table class="docutils align-default">
<colgroup>
<col style="width: 6%" />
<col style="width: 23%" />
<col style="width: 13%" />
<col style="width: 13%" />
<col style="width: 13%" />
<col style="width: 15%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Task</p></th>
<th class="head"><p>Metric</p></th>
<th class="head"><p>Albert-large</p></th>
<th class="head"><p>Albert-xlarge</p></th>
<th class="head"><p>Megatron-345m</p></th>
<th class="head"><p>BERT base paper</p></th>
<th class="head"><p>BERT large paper</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CoLA</p></td>
<td><p>Matthew’s correlation</p></td>
<td><p>54.94</p></td>
<td><p>61.72</p></td>
<td><p>64.56</p></td>
<td><p>52.1</p></td>
<td><p>60.5</p></td>
</tr>
<tr class="row-odd"><td><p>SST-2</p></td>
<td><p>Accuracy</p></td>
<td><p>92.74</p></td>
<td><p>91.86</p></td>
<td><p>95.87</p></td>
<td><p>93.5</p></td>
<td><p>94.9</p></td>
</tr>
<tr class="row-even"><td><p>MRPC</p></td>
<td><p>F1/Accuracy</p></td>
<td><p>92.05/88.97</p></td>
<td><p>91.87/88.61</p></td>
<td><p>92.36/89.46</p></td>
<td><p>88.9/-</p></td>
<td><p>89.3/-</p></td>
</tr>
<tr class="row-odd"><td><p>STS-B</p></td>
<td><p>Person/Spearman corr.</p></td>
<td><p>90.41/90.21</p></td>
<td><p>90.07/90.10</p></td>
<td><p>91.51/91.61</p></td>
<td><p>-/85.8</p></td>
<td><p>-/86.5</p></td>
</tr>
<tr class="row-even"><td><p>QQP</p></td>
<td><p>F1/Accuracy</p></td>
<td><p>88.26/91.26</p></td>
<td><p>88.80/91.65</p></td>
<td><p>89.18/91.91</p></td>
<td><p>71.2/-</p></td>
<td><p>72.1/-</p></td>
</tr>
<tr class="row-odd"><td><p>MNLI</p></td>
<td><p>Matched /Mismatched acc.</p></td>
<td><p>86.69/86.81</p></td>
<td><p>88.66/88.73</p></td>
<td><p>89.86/89.81</p></td>
<td><p>84.6/83.4</p></td>
<td><p>86.7/85.9</p></td>
</tr>
<tr class="row-even"><td><p>QNLI</p></td>
<td><p>Accuracy</p></td>
<td><p>92.68</p></td>
<td><p>93.66</p></td>
<td><p>94.33</p></td>
<td><p>90.5</p></td>
<td><p>92.7</p></td>
</tr>
<tr class="row-odd"><td><p>RTE</p></td>
<td><p>Accuracy</p></td>
<td><p>80.87</p></td>
<td><p>82.86</p></td>
<td><p>83.39</p></td>
<td><p>66.4</p></td>
<td><p>70.1</p></td>
</tr>
</tbody>
</table>
<p>WNLI task was excluded from the experiments due to the problematic WNLI set.
The dev sets were used for evaluation for Albert and Megatron models, and the test sets results for the BERT paper from <a class="bibtex reference internal" href="#nlp-glue-devlin2018bert" id="id9">[NLP-GLUE2]</a>.</p>
<p>Hyperparameters used to get the results from the above table, could be found in the table below.
Each cell in the table represents the following parameters:
Number of GPUs used/ Batch Size/ Learning Rate/ Number of Epochs. For not specified parameters, please refer to the default parameters in the training script.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 14%" />
<col style="width: 27%" />
<col style="width: 29%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Task</p></th>
<th class="head"><p>Albert-large</p></th>
<th class="head"><p>Albert-xlarge</p></th>
<th class="head"><p>Megatron-345m</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CoLA</p></td>
<td><p>1/32/1e-5/3</p></td>
<td><p>1/32/1e-5/10</p></td>
<td><p>4/16/2e-5/12</p></td>
</tr>
<tr class="row-odd"><td><p>SST-2</p></td>
<td><p>4/16/2e-5/5</p></td>
<td><p>4/16/2e-5/12</p></td>
<td><p>4/16/2e-5/12</p></td>
</tr>
<tr class="row-even"><td><p>MRPC</p></td>
<td><p>1/32/1e-5/5</p></td>
<td><p>1/16/2e-5/5</p></td>
<td><p>1/16/2e-5/10</p></td>
</tr>
<tr class="row-odd"><td><p>STS-B</p></td>
<td><p>1/16/2e-5/5</p></td>
<td><p>1/16/4e-5/12</p></td>
<td><p>4/16/3e-5/12</p></td>
</tr>
<tr class="row-even"><td><p>QQP</p></td>
<td><p>1/16/2e-5/5</p></td>
<td><p>4/16/1e-5/12</p></td>
<td><p>4/16/1e-5/12</p></td>
</tr>
<tr class="row-odd"><td><p>MNLI</p></td>
<td><p>4/64/1e-5/5</p></td>
<td><p>4/32/1e-5/5</p></td>
<td><p>4/32/1e-5/5</p></td>
</tr>
<tr class="row-even"><td><p>QNLI</p></td>
<td><p>4/16/1e-5/5</p></td>
<td><p>4/16/1e-5/5</p></td>
<td><p>4/16/2e-5/5</p></td>
</tr>
<tr class="row-odd"><td><p>RTE</p></td>
<td><p>1/16/1e-5/5</p></td>
<td><p>1//16/1e-5/12</p></td>
<td><p>4/16/3e-5/12</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="evaluating-checkpoints">
<h2>Evaluating Checkpoints<a class="headerlink" href="#evaluating-checkpoints" title="Permalink to this headline">¶</a></h2>
<p>During training, the model is evaluated after every epoch and by default a folder named “checkpoints” would be created under the working folder specified by <cite>–work_dir</cite> and checkpoints would be stored there. To do evaluation of a pre-trained checkpoint on a dev set, run the same training script by passing <cite>–checkpoint_dir</cite> and setting <cite>–num_epochs</cite> as zero to avoid the training.
For example, to evaluate a checkpoint trained on MRPC task, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/nlp/glue_benchmark
python glue_benchmark_with_bert.py  <span class="se">\</span>
    --data_dir /path_to_data_dir/MRPC <span class="se">\</span>
    --task_name mrpc <span class="se">\</span>
    --work_dir /path_to_output_folder <span class="se">\</span>
    --pretrained_model_name bert-base-uncased <span class="se">\</span>
    --checkpoint_dir /path_to_output_folder/checkpoints <span class="se">\</span>
    --num_epochs <span class="m">0</span>
</pre></div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-nlp/glue-0"><dl class="citation">
<dt class="bibtex label" id="nlp-glue-cer2017semeval"><span class="brackets"><a class="fn-backref" href="#id6">NLP-GLUE1</a></span></dt>
<dd><p>Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: semantic textual similarity-multilingual and cross-lingual focused evaluation. <em>arXiv preprint arXiv:1708.00055</em>, 2017.</p>
</dd>
<dt class="bibtex label" id="nlp-glue-devlin2018bert"><span class="brackets">NLP-GLUE2</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id9">2</a>)</span></dt>
<dd><p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>, 2018.</p>
</dd>
<dt class="bibtex label" id="nlp-glue-dolan-brockett-2005-automatically"><span class="brackets"><a class="fn-backref" href="#id5">NLP-GLUE3</a></span></dt>
<dd><p>William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In <em>Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</em>. 2005. URL: <a class="reference external" href="https://www.aclweb.org/anthology/I05-5002">https://www.aclweb.org/anthology/I05-5002</a>.</p>
</dd>
<dt class="bibtex label" id="nlp-glue-levesque2012winograd"><span class="brackets"><a class="fn-backref" href="#id8">NLP-GLUE4</a></span></dt>
<dd><p>Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In <em>Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</em>. 2012.</p>
</dd>
<dt class="bibtex label" id="nlp-glue-socher2013recursive"><span class="brackets"><a class="fn-backref" href="#id4">NLP-GLUE5</a></span></dt>
<dd><p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In <em>Proceedings of the 2013 conference on empirical methods in natural language processing</em>, 1631–1642. 2013.</p>
</dd>
<dt class="bibtex label" id="nlp-glue-wang2018glue"><span class="brackets"><a class="fn-backref" href="#id2">NLP-GLUE6</a></span></dt>
<dd><p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: a multi-task benchmark and analysis platform for natural language understanding. arXiv preprint 1804.07461, 2018.</p>
</dd>
<dt class="bibtex label" id="nlp-glue-warstadt2018neural"><span class="brackets"><a class="fn-backref" href="#id3">NLP-GLUE7</a></span></dt>
<dd><p>Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. <em>arXiv preprint arXiv:1805.12471</em>, 2018.</p>
</dd>
<dt class="bibtex label" id="nlp-glue-williams2017broad"><span class="brackets"><a class="fn-backref" href="#id7">NLP-GLUE8</a></span></dt>
<dd><p>Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. <em>arXiv preprint arXiv:1704.05426</em>, 2017.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="joint_intent_slot_filling.html" class="btn btn-neutral float-right" title="Tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="megatron_finetuning.html" class="btn btn-neutral float-left" title="Megatron-LM for Downstream Tasks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2020, NVIDIA

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>