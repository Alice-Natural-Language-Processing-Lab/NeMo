

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nemo.core.neural_graph &mdash; nemo 0.11.0b9 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> nemo
          

          
          </a>

          
            
            
              <div class="version">
                0.11.0b9
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/intro.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../training.html">Fast Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../asr/intro.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../speaker_recognition/intro.html">Speaker Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../speech_command/intro.html">Speech Commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../voice_activity_detection/intro.html">Voice Activity Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nlp/intro.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tts/intro.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../collections/modules.html">NeMo Collections API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api-docs/modules.html">NeMo API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../chinese/intro.html">中文支持</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">nemo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>nemo.core.neural_graph</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for nemo.core.neural_graph</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># =============================================================================</span>
<span class="c1"># Copyright (c) 2020 NVIDIA. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># =============================================================================</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;NeuralGraph&#39;</span><span class="p">,</span>
<span class="p">]</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">OrderedDict</span><span class="p">,</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">os</span> <span class="k">import</span> <span class="n">path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">ruamel.yaml</span> <span class="k">import</span> <span class="n">YAML</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="k">import</span> <span class="n">DataParallel</span>

<span class="kn">from</span> <span class="nn">nemo.backends</span> <span class="k">import</span> <span class="n">get_state_dict</span><span class="p">,</span> <span class="n">load</span><span class="p">,</span> <span class="n">save</span><span class="p">,</span> <span class="n">set_state_dict</span>
<span class="kn">from</span> <span class="nn">nemo.core.neural_factory</span> <span class="k">import</span> <span class="n">DeviceType</span><span class="p">,</span> <span class="n">OperationMode</span>
<span class="kn">from</span> <span class="nn">nemo.core.neural_interface</span> <span class="k">import</span> <span class="n">NeuralInterface</span>
<span class="kn">from</span> <span class="nn">nemo.core.neural_modules</span> <span class="k">import</span> <span class="n">ModuleType</span><span class="p">,</span> <span class="n">NeuralModule</span>
<span class="kn">from</span> <span class="nn">nemo.core.neural_types</span> <span class="k">import</span> <span class="n">NeuralPortNameMismatchError</span><span class="p">,</span> <span class="n">NeuralType</span><span class="p">,</span> <span class="n">NmTensor</span>
<span class="kn">from</span> <span class="nn">nemo.package_info</span> <span class="k">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">nemo_version</span>
<span class="kn">from</span> <span class="nn">nemo.utils</span> <span class="k">import</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">nemo.utils.neural_graph.connection</span> <span class="k">import</span> <span class="n">Connection</span><span class="p">,</span> <span class="n">StepModulePort</span>
<span class="kn">from</span> <span class="nn">nemo.utils.neural_graph.graph_inputs</span> <span class="k">import</span> <span class="n">GraphInputs</span>
<span class="kn">from</span> <span class="nn">nemo.utils.neural_graph.graph_outputs</span> <span class="k">import</span> <span class="n">GraphOutputs</span>

<span class="n">YAML</span> <span class="o">=</span> <span class="n">YAML</span><span class="p">(</span><span class="n">typ</span><span class="o">=</span><span class="s1">&#39;safe&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="NeuralGraph"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph">[docs]</a><span class="k">class</span> <span class="nc">NeuralGraph</span><span class="p">(</span><span class="n">NeuralInterface</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Neural Graph class stores dynamically defined graphs of connected Neural Modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">operation_mode</span><span class="p">:</span> <span class="n">OperationMode</span> <span class="o">=</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">both</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructor. Initializes graph variables.</span>

<span class="sd">        Args:</span>
<span class="sd">            operation_mode: Graph operation mode, that will be propagated along modules during graph creation.</span>
<span class="sd">            [training | eval | both] (DEFAULT: both)</span>
<span class="sd">            name: Name of the graph (optional)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialize the inferface.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Register graph.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span><span class="o">.</span><span class="n">register_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

        <span class="c1"># Store name and operation mode.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_operation_mode</span> <span class="o">=</span> <span class="n">operation_mode</span>

        <span class="c1"># &quot;Modules&quot; - list of modules constituting &quot;nodes&quot; in a given graph.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># All tensors produced within this graph (dict of dicts).</span>
        <span class="c1"># This stores  &quot;all output tensors&quot; dictionary, where the key is the name of &quot;producer&quot; module,</span>
        <span class="c1"># and the value contains a dictionary of all tensors produced by it.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_all_tensors</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># &quot;Steps&quot;: order of the  execution of modules in a graph.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_steps</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>

        <span class="c1"># Bound inputs.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span> <span class="o">=</span> <span class="n">GraphInputs</span><span class="p">()</span>

        <span class="c1"># Bound outputs.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span> <span class="o">=</span> <span class="n">GraphOutputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_all_tensors</span><span class="p">)</span>

        <span class="c1"># Flag indicating whether the &quot;default&quot; output ports/tensors will be automatically bound.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">default_output_binding</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Set default data loader params.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader_config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;dataset&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;shuffle&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;batch_sampler&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;collate_fn&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;timeout&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;worker_init_fn&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;multiprocessing_context&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># Lazy-initialize loader when needed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Data collected during forward propagation.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">{})</span>

        <span class="c1"># Initial device: CPU.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pt_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method &quot;nests&quot; one existing neural graph into another one.</span>
<span class="sd">        Also checks if all inputs were provided and properly connects them.</span>

<span class="sd">        Args:</span>
<span class="sd">            kwargs: keyword arguments containing dictionary of (input_port_name, port_content).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Test operation modes of the nested graphs.</span>
        <span class="n">outer_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span><span class="o">.</span><span class="n">active_graph</span><span class="o">.</span><span class="n">operation_mode</span>
        <span class="n">inner_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">operation_mode</span>

        <span class="k">if</span> <span class="n">inner_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">evaluation</span> <span class="ow">and</span> <span class="n">outer_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Cannot nest &#39;inference&#39; graph into &#39;training&#39;&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inner_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="n">outer_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">evaluation</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Cannot nest &#39;training&#39; graph into &#39;inference&#39;&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inner_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="n">outer_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">both</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Cannot nest &#39;training&#39; graph into &#39;both&#39;&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inner_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">evaluation</span> <span class="ow">and</span> <span class="n">outer_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">both</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Cannot nest &#39;inference&#39; graph into &#39;both&#39;&quot;</span><span class="p">)</span>

        <span class="c1"># Check inputs: iterate through all inputs passed to the &quot;self&quot;.</span>
        <span class="k">for</span> <span class="n">port_name</span><span class="p">,</span> <span class="n">port_content</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Make sure that passed arguments correspond to input port names.</span>
            <span class="k">if</span> <span class="n">port_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">raise</span> <span class="n">NeuralPortNameMismatchError</span><span class="p">(</span><span class="n">port_name</span><span class="p">)</span>

        <span class="c1"># &quot;Nest&quot; this graph into an active graph.</span>
        <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span><span class="o">.</span><span class="n">active_graph</span><span class="o">.</span><span class="n">__nest</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Return output tensors.</span>
        <span class="k">return</span> <span class="n">results</span>

    <span class="k">def</span> <span class="nf">__nest</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inner_graph</span><span class="p">:</span> <span class="s1">&#39;NeuralGraph&#39;</span><span class="p">,</span> <span class="n">inner_graph_args</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method nests (copies) a graph: modules, steps, topology (tensors).</span>

<span class="sd">        Args:</span>
<span class="sd">            inner_graph: Graph to be copied (will be &quot;nested&quot; in this (self) graph).</span>
<span class="sd">            inner_graph_args: inputs passed to the graph call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Remember the number of &quot;already present steps&quot;.</span>
        <span class="n">step_bump</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">steps</span><span class="p">)</span>

        <span class="c1"># &quot;Copy&quot; the modules from nested graph.</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">inner_graph</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Check if module with that name already exists.</span>
            <span class="c1"># TODO: Uncomment when we will refactor all examples so training/validation graphs won&#39;t be added</span>
            <span class="c1"># to the &quot;default&quot; graph.</span>
            <span class="c1"># if key in self._modules.keys():</span>
            <span class="c1">#    raise KeyError(&quot;Neural Graph already contains a module named {}&quot;.format(module.name))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>

        <span class="c1"># Next we should copy the topography - i.e. produce &quot;real&quot; copies of tensors.</span>
        <span class="c1"># In fact, instead of copying, we will produce them, following:</span>
        <span class="c1"># - the execution order defined in &quot;steps&quot;</span>
        <span class="c1"># - connectivity defined in tensor&#39; consumers-ports</span>
        <span class="c1"># (so the same logic that will be used in graph deserialization)</span>

        <span class="c1"># So let us first serialize the connections of the nested graph.</span>
        <span class="c1"># Create a list: (producer.port -&gt; consumer.port)</span>
        <span class="n">inner_connections</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">tensors</span> <span class="ow">in</span> <span class="n">inner_graph</span><span class="o">.</span><span class="n">tensors</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">inner_connections</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">connections</span><span class="p">())</span>

        <span class="c1"># We need to disable the binding of &quot;defeault&quot; ports on per module basis - we will &quot;manually&quot; produce</span>
        <span class="c1"># them only for ports that are already indicated as the &quot;bound&quot; ones in the inner graph.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">default_output_binding</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Now &quot;copy&quot; graph execution order and topology by actually executing each step of the nested graph.</span>
        <span class="k">for</span> <span class="n">step_number</span><span class="p">,</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="n">inner_graph</span><span class="o">.</span><span class="n">steps</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Both module and step will be added by the modules&#39; call().</span>

            <span class="c1"># Get the module.</span>
            <span class="n">module</span> <span class="o">=</span> <span class="n">inner_graph</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span>

            <span class="c1"># Produce list of arguments that will be passed to a given modules.</span>
            <span class="n">module_args</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="c1"># Do it by:</span>
            <span class="c1"># - harvesing input port names of a given module,</span>
            <span class="c1"># - checking if the input was not bound (in the inner graph),</span>
            <span class="c1"># - checking if we have already tensors leading to that input (in outer graph).</span>
            <span class="k">for</span> <span class="n">input_port_name</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">input_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="c1"># Check if this port was bound in the inner graph.</span>
                <span class="n">key</span> <span class="o">=</span> <span class="n">inner_graph</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">has_binding</span><span class="p">(</span><span class="n">step_number</span><span class="p">,</span> <span class="n">input_port_name</span><span class="p">)</span>
                <span class="c1"># If so, then we must pass whatever was passed to that port in the list of arguments.</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">module_args</span><span class="p">[</span><span class="n">input_port_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">inner_graph_args</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                    <span class="c1"># As a result, the &quot;module&quot; call() will bind this input!</span>
                    <span class="k">continue</span>

                <span class="c1"># Else: find a tensor that should be passed to the given module&#39;s input.</span>
                <span class="c1"># Search for producer/port that we should use.</span>
                <span class="k">for</span> <span class="n">connection</span> <span class="ow">in</span> <span class="n">inner_connections</span><span class="p">:</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">connection</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">step_number</span> <span class="o">==</span> <span class="n">step_number</span>
                        <span class="ow">and</span> <span class="n">connection</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">module_name</span> <span class="o">==</span> <span class="n">module_name</span>
                        <span class="ow">and</span> <span class="n">connection</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">port_name</span> <span class="o">==</span> <span class="n">input_port_name</span>
                    <span class="p">):</span>
                        <span class="c1"># Got the connection!</span>
                        <span class="n">bumped_step</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="n">producer</span><span class="o">.</span><span class="n">step_number</span> <span class="o">+</span> <span class="n">step_bump</span>
                        <span class="c1"># producer_name = connection.producer.module_name</span>
                        <span class="n">producer_port_name</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="n">producer</span><span class="o">.</span><span class="n">port_name</span>
                        <span class="k">break</span>

                <span class="c1"># Now, the tensor is already produced in outer (i.e. this) graph!</span>
                <span class="n">module_args</span><span class="p">[</span><span class="n">input_port_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensors</span><span class="p">[</span><span class="n">bumped_step</span><span class="p">][</span><span class="n">producer_port_name</span><span class="p">]</span>

            <span class="c1"># Ok, now we have all keyword arguments. We can call() the module.</span>
            <span class="c1"># This will collect all the produced output tensors and add them to this graph.</span>
            <span class="n">module</span><span class="p">(</span><span class="o">**</span><span class="n">module_args</span><span class="p">)</span>

        <span class="c1"># At that point we have all modules, steps and tensors added to outer (self) graph.</span>
        <span class="c1"># Now we have to prepare the outputs.</span>

        <span class="c1"># This part is different from Neural Module.</span>
        <span class="c1"># Now the goal is NOT to create NEW &quot;tensors&quot;, but to return the BOUND ones!</span>
        <span class="c1"># Still, those must be bound in the outer (active) graph, but using port names from the inner (nested) graph.</span>

        <span class="c1"># Get list of &quot;the adequate output tensors&quot;.</span>
        <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Iterate through outputs of the inner graph.</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inner_graph</span><span class="o">.</span><span class="n">output_tensors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Find the tensors within this (outer) graph that are outputs by the same producer-port.</span>
            <span class="n">bumped_step</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">producer_step_number</span> <span class="o">+</span> <span class="n">step_bump</span>
            <span class="c1"># producer_name = tensor.producer_name</span>
            <span class="n">producer_port_name</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">name</span>
            <span class="c1"># Get adequate tensor from &quot;outer graph&quot; (self).</span>
            <span class="n">output_tensors</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensors</span><span class="p">[</span><span class="n">bumped_step</span><span class="p">][</span><span class="n">producer_port_name</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Return a single tensor.</span>
            <span class="n">key</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">output_tensors</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

            <span class="c1"># Bind the &quot;default&quot; output ports of the inner graph as &quot;default&quot; output ports of this graph.</span>
            <span class="c1"># Call the bind() method of bound_outputs directly, as we already have the tensors in our graph.</span>
            <span class="c1"># But: Use output port name of the inner graph!</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">bind</span><span class="p">([</span><span class="n">results</span><span class="p">],</span> <span class="p">[</span><span class="n">key</span><span class="p">])</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Create a named tuple type enabling to access outputs by attributes (e.g. out.x).</span>
            <span class="n">output_class_name</span> <span class="o">=</span> <span class="n">f</span><span class="s1">&#39;</span><span class="si">{self.__class__.__name__}</span><span class="s1">Output&#39;</span>
            <span class="n">result_type</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="n">typename</span><span class="o">=</span><span class="n">output_class_name</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="n">output_tensors</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

            <span class="c1"># Return the bound output tensors.</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">result_type</span><span class="p">(</span><span class="o">*</span><span class="n">output_tensors</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

            <span class="c1"># Bind the &quot;default&quot; output ports of the inner graph as &quot;default&quot; output ports of this graph.</span>
            <span class="c1"># Call the bind() method of bound_outputs directly, as we already have the tensors in our graph.</span>
            <span class="c1"># But: Use output port name of the inner graph!</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">output_tensors</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">output_tensors</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="c1"># Ok, now we can turn automatic binding on.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">default_output_binding</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Return the results.</span>
        <span class="k">return</span> <span class="n">results</span>

<div class="viewcode-block" id="NeuralGraph.record_step"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.record_step">[docs]</a>    <span class="k">def</span> <span class="nf">record_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">NeuralModule</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Records the operation (the module to be executed) on a list.</span>

<span class="sd">        Args:</span>
<span class="sd">            module: Neural modules added to a given graph.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Step number.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># The solution allows loops in the graph.</span>
        <span class="c1"># This also means that module with that name can already be present in the graph.</span>
        <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="c1"># Check if this is the same module.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">module</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;Neural Graph already contains a different module with name `</span><span class="si">{}</span><span class="s2">`!&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Add module to list of modules.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>

        <span class="c1"># Add step - store the module name.</span>
        <span class="n">step_number</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_steps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_steps</span><span class="p">[</span><span class="n">step_number</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">name</span>

        <span class="c1"># Return the current step number.</span>
        <span class="k">return</span> <span class="n">step_number</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">step_number</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            The current step number.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_steps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<div class="viewcode-block" id="NeuralGraph.bind_outputs"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.bind_outputs">[docs]</a>    <span class="k">def</span> <span class="nf">bind_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors_list</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">NmTensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">NmTensor</span><span class="p">]]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Binds the output tensors.</span>

<span class="sd">        Args:</span>
<span class="sd">            tensors_list: A single tensor OR a List of tensors to be bound.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Handle both single port and lists of ports to be bound.</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">tensors_list</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
            <span class="n">tensors_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensors_list</span><span class="p">]</span>

        <span class="c1"># Add tensors to list of list of tensors.</span>
        <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors_list</span><span class="p">:</span>
            <span class="c1"># Add tensor to &quot;all&quot; tensors dictionary.</span>
            <span class="n">step_number</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">producer_step_number</span>
            <span class="k">if</span> <span class="n">step_number</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_tensors</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_all_tensors</span><span class="p">[</span><span class="n">step_number</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="n">port_name</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">name</span>
            <span class="c1"># Add tensor.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_all_tensors</span><span class="p">[</span><span class="n">step_number</span><span class="p">][</span><span class="n">port_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>

        <span class="c1"># Bind the tensors as graph outputs.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_output_binding</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">tensors_list</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphInputs</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            Graph input.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">input_ports</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NeuralType</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns definitions of graph input ports (dict of Neural Types).</span>

<span class="sd">        .. note::</span>
<span class="sd">            This method actually returns an immutable  dictionary with port types (like Neural Modules).</span>
<span class="sd">            In order to get access to actual graph inputs please call the inputs() method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Graph input ports definitions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span><span class="o">.</span><span class="n">definitions</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphOutputs</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns graph outputs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Graph outputs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_ports</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NeuralType</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns definitions of module output ports (dict of Neural Types).</span>

<span class="sd">        .. note::</span>
<span class="sd">            This method actually returns an immutable dictionary with port types (like Neural Modules).</span>
<span class="sd">            In order to get access to actual graph outpus please call the outputs() method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Graph output ports definitions.</span>
<span class="sd">            </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span><span class="o">.</span><span class="n">definitions</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NmTensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            Fraph output tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span><span class="o">.</span><span class="n">tensors</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NeuralModule</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot; Returns modules. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralModule</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; Returns module given its name (name of the variable).</span>

<span class="sd">            Args:</span>
<span class="sd">                key: Name of the variable.</span>
<span class="sd">            </span>
<span class="sd">            Raises:</span>
<span class="sd">                KeyError: Neural Graph doesn&#39;t contain a module with a given name (key).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;Neural Graph doesn&#39;t contain a module named </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            The number of modules (vertices) in a given graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">steps</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            Dictionary [steps_number, module_name]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_steps</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property returning a (double) dictionary of all output tensors.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary of tensors in the format [module_name][output_port_name].</span>
<span class="sd">         &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_tensors</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tensor_list</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">NmTensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property returning output tensors by extracting them on the fly from the bound outputs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List of tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Get tensors by acessing the producer-ports.</span>
        <span class="k">for</span> <span class="n">tensors_per_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_tensors</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors_per_module</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="c1"># Add it to the list.</span>
                <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="c1"># Return the result.</span>
        <span class="k">return</span> <span class="n">tensor_list</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">operation_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OperationMode</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            Operation mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_operation_mode</span>

    <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;NeuralGraph&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">        Activates this graph.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The graph object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span><span class="o">.</span><span class="n">active_graph</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">exc_traceback</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deactivates the current graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span><span class="o">.</span><span class="n">active_graph</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="NeuralGraph.activate"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.activate">[docs]</a>    <span class="k">def</span> <span class="nf">activate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">        Activates this graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span><span class="o">.</span><span class="n">active_graph</span> <span class="o">=</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="NeuralGraph.deactivate"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.deactivate">[docs]</a>    <span class="k">def</span> <span class="nf">deactivate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deactivates the current graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span><span class="o">.</span><span class="n">active_graph</span> <span class="o">=</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="NeuralGraph.export_to_config"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.export_to_config">[docs]</a>    <span class="k">def</span> <span class="nf">export_to_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Exports the neural graph to a file.</span>

<span class="sd">        Args:</span>
<span class="sd">            config_file: Name (and path) of the config file (YML) to be written to.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Greate an absolute path.</span>
        <span class="n">abs_path_file</span> <span class="o">=</span> <span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>

        <span class="c1"># Serialize the graph.</span>
        <span class="n">to_export</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">serialize</span><span class="p">()</span>

        <span class="c1"># All parameters are ok, let&#39;s export.</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">abs_path_file</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
            <span class="n">YAML</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">to_export</span><span class="p">,</span> <span class="n">outfile</span><span class="p">)</span>

        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Configuration of graph `</span><span class="si">{}</span><span class="s2">` (</span><span class="si">{}</span><span class="s2">) exported to &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">abs_path_file</span><span class="p">)</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="NeuralGraph.serialize"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.serialize">[docs]</a>    <span class="k">def</span> <span class="nf">serialize</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method serializes the whole graph.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary containing description of the whole graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Create a dictionary representing the serialized object.</span>
        <span class="n">serialized_graph</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Add &quot;header&quot; with module &quot;specification&quot;.</span>
        <span class="n">serialized_graph</span><span class="p">[</span><span class="s2">&quot;header&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__serialize_header</span><span class="p">()</span>

        <span class="c1"># Add modules.</span>
        <span class="n">serialized_graph</span><span class="p">[</span><span class="s2">&quot;modules&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__serialize_modules</span><span class="p">()</span>

        <span class="c1"># Add steps.</span>
        <span class="n">serialized_graph</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__serialize_steps</span><span class="p">()</span>

        <span class="c1"># Add connectinos.</span>
        <span class="n">serialized_graph</span><span class="p">[</span><span class="s2">&quot;connections&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__serialize_connections</span><span class="p">()</span>

        <span class="c1"># Serialize graph (bound) inputs.</span>
        <span class="n">serialized_graph</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span><span class="o">.</span><span class="n">serialize</span><span class="p">()</span>

        <span class="c1"># Serialize graph (bound) outputs.</span>
        <span class="n">serialized_graph</span><span class="p">[</span><span class="s2">&quot;outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span><span class="o">.</span><span class="n">serialize</span><span class="p">()</span>

        <span class="c1"># Return the dictionary.</span>
        <span class="k">return</span> <span class="n">serialized_graph</span></div>

    <span class="k">def</span> <span class="nf">__serialize_header</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private method responsible for serializing the graph header.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary containing description of the whole graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Generate full_spec of the class.</span>
        <span class="n">full_spec</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__module__</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__qualname__</span><span class="p">)</span>
        <span class="n">header</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;nemo_core_version&quot;</span><span class="p">:</span> <span class="n">nemo_version</span><span class="p">,</span> <span class="s2">&quot;full_spec&quot;</span><span class="p">:</span> <span class="n">full_spec</span><span class="p">}</span>
        <span class="c1"># Add operation mode.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_operation_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">header</span><span class="p">[</span><span class="s2">&quot;operation_mode&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;training&quot;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_operation_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">evaluation</span><span class="p">:</span>
            <span class="n">header</span><span class="p">[</span><span class="s2">&quot;operation_mode&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;inference&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">header</span><span class="p">[</span><span class="s2">&quot;operation_mode&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;both&quot;</span>
        <span class="c1"># Return header.</span>
        <span class="k">return</span> <span class="n">header</span>

    <span class="k">def</span> <span class="nf">__serialize_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private method responsible for serializing the modules present in the graph.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary containing description of all graph modules.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">serialized_modules</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">serialized_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">serialize</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">serialized_modules</span>

    <span class="k">def</span> <span class="nf">__serialize_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private method responsible for serializing the steps (order of module executions).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary containing description of the steps.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">serialized_steps</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">no</span><span class="p">,</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_steps</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">serialized_steps</span><span class="p">[</span><span class="n">no</span><span class="p">]</span> <span class="o">=</span> <span class="n">module_name</span>
        <span class="k">return</span> <span class="n">serialized_steps</span>

    <span class="k">def</span> <span class="nf">__serialize_connections</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private method responsible for serializing the connections in the graph.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List containing &quot;connections&quot; between modules.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">serialized_connections</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Iterate through &quot;tensor modules&quot;.</span>
        <span class="k">for</span> <span class="n">tensors</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_tensors</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="c1"># Iterate through &quot;tensor output ports&quot;.</span>
            <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="c1"># &quot;Transform&quot; tensor to the list of connections.</span>
                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">tensor</span><span class="o">.</span><span class="n">connections</span><span class="p">():</span>
                    <span class="c1"># Serialize!</span>
                    <span class="n">source</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">producer</span><span class="o">.</span><span class="n">step_number</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">c</span><span class="o">.</span><span class="n">producer</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">c</span><span class="o">.</span><span class="n">producer</span><span class="o">.</span><span class="n">port_name</span>
                    <span class="n">target</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">step_number</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">c</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">c</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">port_name</span>
                    <span class="n">ntype_str</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">ntype</span><span class="p">)</span>
                    <span class="n">serialized_connections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span> <span class="o">+</span> <span class="s2">&quot;-&gt;&quot;</span> <span class="o">+</span> <span class="n">target</span> <span class="o">+</span> <span class="s2">&quot; | &quot;</span> <span class="o">+</span> <span class="n">ntype_str</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">serialized_connections</span>

<div class="viewcode-block" id="NeuralGraph.import_from_config"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.import_from_config">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">import_from_config</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">config_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">reuse_existing_modules</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">overwrite_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;NeuralGraph&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Class method importing the neural graph from the configuration file.</span>
<span class="sd">        Raises an ImportError exception when config file is invalid.</span>

<span class="sd">        Args:</span>
<span class="sd">            config_file: path (absolute or relative) and name of the config file (YML)</span>
<span class="sd">            reuse_existing_modules: If the modules with (name, type, init_params) are already created, import will</span>
<span class="sd">            connect to them instead of creating new instances.</span>
<span class="sd">            overwrite_params: Dictionary containing parameters that will be added to or overwrite (!) the default</span>
<span class="sd">            parameters loaded from the configuration file</span>
<span class="sd">            name: Name of the new graph (optional, DEFAULT: NONE)</span>
<span class="sd">        Returns:</span>
<span class="sd">            Instance of the created NeuralGraph object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loading configuration of a new Neural Graph from the `</span><span class="si">{}</span><span class="s2">` file&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">config_file</span><span class="p">))</span>

        <span class="c1"># Validate the content of the configuration file (its header).</span>
        <span class="n">loaded_config</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">__validate_config_file</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>
        <span class="c1"># TODO: overwrite params?</span>

        <span class="c1"># &quot;Deserialize&quot; the graph.</span>
        <span class="n">new_graph</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">loaded_config</span><span class="p">,</span> <span class="n">reuse_existing_modules</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

        <span class="c1"># Return the object.</span>
        <span class="k">return</span> <span class="n">new_graph</span></div>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__validate_config_file</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Class method validating whether the config file has a proper content (sections, specification etc.).</span>
<span class="sd">        Raises an ImportError exception when config file is invalid or</span>
<span class="sd">        incompatible (when called from a particular class).</span>

<span class="sd">        Args:</span>
<span class="sd">            config_file: path (absolute or relative) and name of the config file (YML)</span>
<span class="sd">        Returns:</span>
<span class="sd">            A loaded configuration file (dictionary).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Greate an absolute path.</span>
        <span class="n">abs_path_file</span> <span class="o">=</span> <span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>

        <span class="c1"># Open the config file.</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">abs_path_file</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">stream</span><span class="p">:</span>
            <span class="n">loaded_config</span> <span class="o">=</span> <span class="n">YAML</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>

        <span class="c1"># Check sections.</span>
        <span class="k">for</span> <span class="n">section_name</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;header&quot;</span><span class="p">,</span> <span class="s2">&quot;modules&quot;</span><span class="p">,</span> <span class="s2">&quot;steps&quot;</span><span class="p">,</span> <span class="s2">&quot;connections&quot;</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;outputs&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">section_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">loaded_config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                    <span class="s2">&quot;The loaded config `</span><span class="si">{}</span><span class="s2">` doesn&#39;t contain the required `</span><span class="si">{}</span><span class="s2">` section&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">config_file</span><span class="p">,</span> <span class="n">section_name</span>
                    <span class="p">)</span>
                <span class="p">)</span>

        <span class="c1"># Parse the &quot;full specification&quot;.</span>
        <span class="n">spec_list</span> <span class="o">=</span> <span class="n">loaded_config</span><span class="p">[</span><span class="s2">&quot;header&quot;</span><span class="p">][</span><span class="s2">&quot;full_spec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if config contains definition of Neural Graph.</span>
        <span class="k">if</span> <span class="n">spec_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;NeuralGraph&quot;</span><span class="p">:</span>
            <span class="n">txt</span> <span class="o">=</span> <span class="s2">&quot;The loaded file `</span><span class="si">{}</span><span class="s2">` contains configuration of &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>
            <span class="n">txt</span> <span class="o">=</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;`</span><span class="si">{}</span><span class="s2">` thus cannot be used for instantiation of Neural Graph&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">spec_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>

        <span class="c1"># Success - return the loaded configuration.</span>
        <span class="k">return</span> <span class="n">loaded_config</span>

<div class="viewcode-block" id="NeuralGraph.deserialize"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.deserialize">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">deserialize</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">configuration</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">reuse_existing_modules</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;NeuralGraph&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Class method creating a graph instance by deserializing the provided configuratino.</span>

<span class="sd">        Args:</span>
<span class="sd">            configuration: Dictionary containing serialized graph.</span>
<span class="sd">            reuse_existing_modules: If the modules with (name, type, init_params) are already created, import will</span>
<span class="sd">            connect to them instead of creating new instances.</span>
<span class="sd">        Returns:</span>
<span class="sd">            Instance of the created NeuralGraph object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Deserialize header and get object class.</span>
        <span class="n">operation_mode</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">__deserialize_header</span><span class="p">(</span><span class="n">configuration</span><span class="p">[</span><span class="s2">&quot;header&quot;</span><span class="p">])</span>

        <span class="c1"># Create the graph instance.</span>
        <span class="n">new_graph</span> <span class="o">=</span> <span class="n">NeuralGraph</span><span class="p">(</span><span class="n">operation_mode</span><span class="o">=</span><span class="n">operation_mode</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Instantiated a new Neural Graph named `</span><span class="si">{}</span><span class="s2">` with mode `</span><span class="si">{}</span><span class="s2">`&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">new_graph</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">new_graph</span><span class="o">.</span><span class="n">operation_mode</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># Deserialize modules.</span>
        <span class="n">modules</span> <span class="o">=</span> <span class="n">new_graph</span><span class="o">.</span><span class="n">__deserialize_modules</span><span class="p">(</span><span class="n">configuration</span><span class="p">[</span><span class="s2">&quot;modules&quot;</span><span class="p">],</span> <span class="n">reuse_existing_modules</span><span class="p">)</span>

        <span class="c1"># Deserialize steps.</span>
        <span class="n">steps</span> <span class="o">=</span> <span class="n">new_graph</span><span class="o">.</span><span class="n">__deserialize_steps</span><span class="p">(</span><span class="n">configuration</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">])</span>

        <span class="c1"># Deserialize the connections between modules.</span>
        <span class="n">connections</span> <span class="o">=</span> <span class="n">new_graph</span><span class="o">.</span><span class="n">__deserialize_connections</span><span class="p">(</span><span class="n">configuration</span><span class="p">[</span><span class="s2">&quot;connections&quot;</span><span class="p">],</span> <span class="n">modules</span><span class="p">)</span>

        <span class="c1"># Deserialize input bindings - return it in an external entity.</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">GraphInputs</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">configuration</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <span class="n">modules</span><span class="p">)</span>

        <span class="c1"># Deserialize &quot;manual&quot; output bindings.</span>
        <span class="n">new_graph</span><span class="o">.</span><span class="n">_outputs</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">configuration</span><span class="p">[</span><span class="s2">&quot;outputs&quot;</span><span class="p">],</span> <span class="n">modules</span><span class="p">)</span>

        <span class="c1"># Now we have to execute the graph, following the steps and connections.</span>
        <span class="n">new_graph</span><span class="o">.</span><span class="n">__execute_and_create_tensors</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">modules</span><span class="p">,</span> <span class="n">connections</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># Return the graph instance.</span>
        <span class="k">return</span> <span class="n">new_graph</span></div>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__deserialize_header</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">serialized_header</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private class method deserializing the header and extracts the general information.</span>

<span class="sd">        Args:</span>
<span class="sd">            serialized_header: Dictionary containing graph header.</span>
<span class="sd">        Returns:</span>
<span class="sd">            Operation mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Parse the &quot;full specification&quot; - do not need that now.</span>
        <span class="c1"># spec_list = serialized_header[&quot;full_spec&quot;].split(&quot;.&quot;)</span>

        <span class="c1"># Get operation mode.</span>
        <span class="k">if</span> <span class="n">serialized_header</span><span class="p">[</span><span class="s2">&quot;operation_mode&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;training&quot;</span><span class="p">:</span>
            <span class="n">operation_mode</span> <span class="o">=</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">training</span>
        <span class="k">elif</span> <span class="n">serialized_header</span><span class="p">[</span><span class="s2">&quot;operation_mode&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;inference&quot;</span><span class="p">:</span>
            <span class="n">operation_mode</span> <span class="o">=</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">evaluation</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">operation_mode</span> <span class="o">=</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">both</span>

        <span class="c1"># Return the mode.</span>
        <span class="k">return</span> <span class="n">operation_mode</span>

    <span class="k">def</span> <span class="nf">__deserialize_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">serialized_modules</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">reuse_existing_modules</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private method deserializing the modules present in the graph.</span>

<span class="sd">        Args:</span>
<span class="sd">            serialized_modules: Dictionary containing graph modules.</span>
<span class="sd">            reuse_existing_modules: If True, won create a new module when a module with a given name exists.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary of modules.</span>

<span class="sd">        Raises:</span>
<span class="sd">            KeyError: A module with name already exists (if reuse_existing_modules is set to False).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">modules</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module_params</span> <span class="ow">in</span> <span class="n">serialized_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Check if module already exists.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">has</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
                <span class="c1"># Check if we can reuse the existing modules.</span>
                <span class="k">if</span> <span class="n">reuse_existing_modules</span><span class="p">:</span>
                    <span class="n">modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;A module with name `</span><span class="si">{}</span><span class="s2">` already exists!&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Ok, create a new module.</span>
                <span class="n">modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">NeuralModule</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">module_params</span><span class="p">)</span>
        <span class="c1"># Ok, done.</span>
        <span class="k">return</span> <span class="n">modules</span>

    <span class="k">def</span> <span class="nf">__deserialize_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">serialized_steps</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private method deserializing the steps (order of module executions).</span>

<span class="sd">        Args:</span>
<span class="sd">            serialized_steps: Dictionary containing serialized steps.</span>
<span class="sd">        Returns:</span>
<span class="sd">            Odered dict with steps.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">steps</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">serialized_steps</span><span class="p">)):</span>
            <span class="n">steps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">serialized_steps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="c1"># Ok, done.</span>
        <span class="k">return</span> <span class="n">steps</span>

    <span class="k">def</span> <span class="nf">__deserialize_connections</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">serialized_connections</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">modules</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NeuralModule</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private method deserializing the connections in the graph.</span>

<span class="sd">        Args:</span>
<span class="sd">            serialized_steps: Dictionary containing serialized connections.</span>
<span class="sd">            modules: List of modules.</span>
<span class="sd">        Returns:</span>
<span class="sd">            List of connections, in a format enabling graph traversing.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">connections</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Deserialize connections one by one.</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">serialized_connections</span><span class="p">:</span>
            <span class="c1"># Deserialize!</span>
            <span class="p">[</span><span class="n">producer</span><span class="p">,</span> <span class="n">consumer_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">)</span>
            <span class="p">[</span><span class="n">consumer</span><span class="p">,</span> <span class="n">ntype_str</span><span class="p">]</span> <span class="o">=</span> <span class="n">consumer_type</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; | &quot;</span><span class="p">)</span>
            <span class="p">[</span><span class="n">producer_step</span><span class="p">,</span> <span class="n">producer_name</span><span class="p">,</span> <span class="n">producer_port_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">producer</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
            <span class="p">[</span><span class="n">consumer_step</span><span class="p">,</span> <span class="n">consumer_name</span><span class="p">,</span> <span class="n">consumer_port_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">consumer</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
            <span class="n">producer_mp</span> <span class="o">=</span> <span class="n">StepModulePort</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">producer_step</span><span class="p">),</span> <span class="n">producer_name</span><span class="p">,</span> <span class="n">producer_port_name</span><span class="p">)</span>
            <span class="n">consumer_mp</span> <span class="o">=</span> <span class="n">StepModulePort</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">consumer_step</span><span class="p">),</span> <span class="n">consumer_name</span><span class="p">,</span> <span class="n">consumer_port_name</span><span class="p">)</span>
            <span class="c1"># Get tensor type.</span>
            <span class="n">ntype</span> <span class="o">=</span> <span class="n">modules</span><span class="p">[</span><span class="n">producer_name</span><span class="p">]</span><span class="o">.</span><span class="n">output_ports</span><span class="p">[</span><span class="n">producer_port_name</span><span class="p">]</span>
            <span class="c1"># Validate if neural type is ok.</span>
            <span class="k">assert</span> <span class="n">ntype_str</span> <span class="o">==</span> <span class="nb">str</span><span class="p">(</span><span class="n">ntype</span><span class="p">)</span>

            <span class="c1"># Add connection.</span>
            <span class="n">connections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Connection</span><span class="p">(</span><span class="n">producer_mp</span><span class="p">,</span> <span class="n">consumer_mp</span><span class="p">,</span> <span class="n">ntype</span><span class="p">))</span>
        <span class="c1"># Ok, done.</span>
        <span class="k">return</span> <span class="n">connections</span>

    <span class="k">def</span> <span class="nf">__execute_and_create_tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">modules</span><span class="p">,</span> <span class="n">connections</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method creates (internal) tensors of the graph by executing it following the order and using</span>
<span class="sd">        the provided connections and inputs.</span>

<span class="sd">        Args:</span>
<span class="sd">            steps: List of steps to be executed.</span>
<span class="sd">            modules: List of modules.</span>
<span class="sd">            connections: List of connections.</span>
<span class="sd">            inputs: List of &quot;bound inputs&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Activate this graph, so all the tensors will be added to this !</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activate</span><span class="p">()</span>

        <span class="c1"># We need to disable the binding of &quot;defeault&quot; ports on per module basis.</span>
        <span class="c1"># We will &quot;manually&quot; produce (e.g. deserialize) them outside of this function.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">default_output_binding</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Now &quot;copy&quot; graph execution order and topology by actually executing each step of the nested graph.</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="n">steps</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Both module and step will be added by the modules&#39; call().</span>

            <span class="c1"># Get the module.</span>
            <span class="n">module</span> <span class="o">=</span> <span class="n">modules</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span>

            <span class="c1"># Produce list of arguments that will be passed to a given module.</span>
            <span class="n">module_args</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="c1"># Do it by:</span>
            <span class="c1"># - harvesing input port names of a given module,</span>
            <span class="c1"># - checking if the input was not bound (in the inner graph),</span>
            <span class="c1"># - checking if we have already tensors leading to that input (in outer graph).</span>
            <span class="k">for</span> <span class="n">input_port_name</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">input_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="c1"># Check if this port was bound in the inner graph.</span>
                <span class="n">key</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">has_binding</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">input_port_name</span><span class="p">)</span>

                <span class="c1"># import pdb;pdb.set_trace()</span>
                <span class="c1"># If so, then we must pass the binding!</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># Copy the port &quot;definition&quot; (i.e. is NeuralType) using the same port name.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

                    <span class="c1"># Pass this object to module input argument.</span>
                    <span class="n">module_args</span><span class="p">[</span><span class="n">input_port_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

                <span class="c1"># Else: find a tensor that should be passed to the given module&#39;s input.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Search for producer/port that we should use.</span>
                    <span class="k">for</span> <span class="n">connection</span> <span class="ow">in</span> <span class="n">connections</span><span class="p">:</span>
                        <span class="k">if</span> <span class="p">(</span>
                            <span class="n">connection</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">step_number</span> <span class="o">==</span> <span class="n">step</span>
                            <span class="ow">and</span> <span class="n">connection</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">module_name</span> <span class="o">==</span> <span class="n">module_name</span>
                            <span class="ow">and</span> <span class="n">connection</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">port_name</span> <span class="o">==</span> <span class="n">input_port_name</span>
                        <span class="p">):</span>
                            <span class="c1"># Got the connection!</span>
                            <span class="n">producer_step_number</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="n">producer</span><span class="o">.</span><span class="n">step_number</span>
                            <span class="c1"># producer_name = connection.producer.module_name</span>
                            <span class="n">producer_port_name</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="n">producer</span><span class="o">.</span><span class="n">port_name</span>
                            <span class="k">break</span>
                    <span class="c1"># Now, the tensor is already produced in outer (i.e. this) graph!</span>
                    <span class="n">module_args</span><span class="p">[</span><span class="n">input_port_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensors</span><span class="p">[</span><span class="n">producer_step_number</span><span class="p">][</span><span class="n">producer_port_name</span><span class="p">]</span>
                <span class="c1"># End: for</span>

            <span class="c1"># Ok, now we have all keyword arguments. We can call() the module.</span>
            <span class="c1"># This will collect all the produced output tensors and add them to this graph.</span>
            <span class="n">module</span><span class="p">(</span><span class="o">**</span><span class="n">module_args</span><span class="p">)</span>

        <span class="c1"># At that point we have all modules, steps and tensors added to outer (self) graph.</span>
        <span class="c1"># Now we have to prepare the outputs.</span>

        <span class="c1"># Deactivate graph.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deactivate</span><span class="p">()</span>

        <span class="c1"># Ok, now we can turn automatic binding on.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">default_output_binding</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="NeuralGraph.summary"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.summary">[docs]</a>    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">        Returns:</span>
<span class="sd">            A nice, full graph summary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Line &quot;decorator&quot;.</span>
        <span class="n">desc</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="mi">113</span> <span class="o">*</span> <span class="s1">&#39;=&#39;</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="c1"># 1. general information.</span>
        <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot;The `</span><span class="si">{}</span><span class="s2">` Neural Graph [</span><span class="si">{}</span><span class="s2">, &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">operation_mode</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_complete</span><span class="p">:</span>
            <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot; COMPLETE,&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot; INCOMPLETE,&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_trainable</span><span class="p">:</span>
            <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot; TRAINABLE]:</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot; NON-TRAINABLE]:</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="c1"># 2. modules.</span>
        <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot; * Modules (</span><span class="si">{}</span><span class="s2">):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">trainable</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">is_frozen</span><span class="p">():</span>
                <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot;    * `</span><span class="si">{}</span><span class="s2">` (</span><span class="si">{}</span><span class="s2">) [FROZEN]</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot;    * `</span><span class="si">{}</span><span class="s2">` (</span><span class="si">{}</span><span class="s2">)</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

        <span class="c1"># 3. steps.</span>
        <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot; * Steps (</span><span class="si">{}</span><span class="s2">):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_steps</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">num</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_steps</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot;    </span><span class="si">{}</span><span class="s2">. </span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>

        <span class="c1"># 4. connections.</span>
        <span class="n">connections</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__serialize_connections</span><span class="p">()</span>
        <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot; * Connections (</span><span class="si">{}</span><span class="s2">):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">connections</span><span class="p">))</span>
        <span class="c1"># if len(connections) == 0:</span>
        <span class="c1">#    desc += &quot;    -\n&quot;</span>
        <span class="k">for</span> <span class="n">connection</span> <span class="ow">in</span> <span class="n">connections</span><span class="p">:</span>
            <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot;    * </span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">connection</span><span class="p">)</span>

        <span class="c1"># 5. graph (bound) inputs.</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span><span class="o">.</span><span class="n">serialize</span><span class="p">()</span>
        <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot; * Graph Inputs (</span><span class="si">{}</span><span class="s2">):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="c1"># if len(inputs) == 0:</span>
        <span class="c1">#    desc += &quot;    -\n&quot;</span>
        <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot;    * </span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

        <span class="c1"># 6. graph (bound) outputs.</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span><span class="o">.</span><span class="n">serialize</span><span class="p">()</span>
        <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot; * Graph Outputs (</span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;mappings&quot;</span><span class="p">]),</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">])</span>
        <span class="c1"># if len(outputs) == 0:</span>
        <span class="c1">#    desc += &quot;    -\n&quot;</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;mappings&quot;</span><span class="p">]:</span>
            <span class="n">desc</span> <span class="o">+=</span> <span class="s2">&quot;    * </span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="c1"># Line &quot;decorator&quot;.</span>
        <span class="n">desc</span> <span class="o">+=</span> <span class="mi">113</span> <span class="o">*</span> <span class="s1">&#39;=&#39;</span>

        <span class="c1"># Return the result.</span>
        <span class="k">return</span> <span class="n">desc</span></div>

<div class="viewcode-block" id="NeuralGraph.freeze"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.freeze">[docs]</a>    <span class="k">def</span> <span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A method that freezes the weights of the trainable modules in a graph.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_names: List of modules to be frozen (Optional). If passed, all modules will be unfrozen.</span>
<span class="sd">        Raises:</span>
<span class="sd">            KeyError: If name of the module won&#39;t be recognized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Work on all modules.</span>
        <span class="k">if</span> <span class="n">module_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

        <span class="c1"># Iterate through modules one by one.</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">module_names</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;Module `</span><span class="si">{}</span><span class="s2">` not present in the `</span><span class="si">{}</span><span class="s2">` graph&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
            <span class="c1"># Check module type.</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
                <span class="c1"># Freeze weights of the module.</span>
                <span class="n">module</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Module `</span><span class="si">{}</span><span class="s2">` is not trainable so cannot be frozen&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span></div>

<div class="viewcode-block" id="NeuralGraph.unfreeze"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.unfreeze">[docs]</a>    <span class="k">def</span> <span class="nf">unfreeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Unfreezes weights of the trainable modules in a graph.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_names: List of modules to be unfrozen (Optional). If not passed, all modules will be unfrozen.</span>
<span class="sd">        Raises:</span>
<span class="sd">            KeyError: If name of the module won&#39;t be recognized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Work on all modules.</span>
        <span class="k">if</span> <span class="n">module_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

        <span class="c1"># Iterate through modules one by one.</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">module_names</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;Module `</span><span class="si">{}</span><span class="s2">` not present in the `</span><span class="si">{}</span><span class="s2">` graph&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
            <span class="c1"># Check module type.</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
                <span class="c1"># Unfreeze weights of the module.</span>
                <span class="n">module</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Module `</span><span class="si">{}</span><span class="s2">` is not trainable so cannot be unfrozen&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span></div>

<div class="viewcode-block" id="NeuralGraph.save_to"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.save_to">[docs]</a>    <span class="k">def</span> <span class="nf">save_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves the state of trainable modules in the graph to a checkpoint file.</span>

<span class="sd">        Args:</span>
<span class="sd">            filename (string): Name of the file where the checkpoint will be saved.</span>
<span class="sd">            module_names: List of modules to be frozen (Optional). If passed, all modules will be saved.</span>
<span class="sd">        Raises:</span>
<span class="sd">            KeyError: If name of the module won&#39;t be recognized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Work on all modules.</span>
        <span class="k">if</span> <span class="n">module_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

        <span class="c1"># Prepare the &quot;graph checkpoint&quot;.</span>
        <span class="n">chkpt</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;header&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;nemo_core_version&quot;</span><span class="p">:</span> <span class="n">nemo_version</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">},</span> <span class="s2">&quot;modules&quot;</span><span class="p">:</span> <span class="p">{}}</span>

        <span class="n">log_str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
        <span class="c1"># Iterate through the modules one by one.</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">module_names</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;Module `</span><span class="si">{}</span><span class="s2">` not present in the `</span><span class="si">{}</span><span class="s2">` graph&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
            <span class="c1"># Check module type.</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
                <span class="c1"># Get module state_dict().</span>
                <span class="n">chkpt</span><span class="p">[</span><span class="s2">&quot;modules&quot;</span><span class="p">][</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_state_dict</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
                <span class="n">log_str</span> <span class="o">+=</span> <span class="s2">&quot;  * Module &#39;</span><span class="si">{}</span><span class="s2">&#39; (</span><span class="si">{}</span><span class="s2">) params saved </span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Module `</span><span class="si">{}</span><span class="s2">` is not trainable so cannot be saved&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>

        <span class="c1"># Save checkpoint.</span>
        <span class="n">save</span><span class="p">(</span><span class="n">chkpt</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
        <span class="n">log_str</span> <span class="o">=</span> <span class="s2">&quot;Saved  the &#39;</span><span class="si">{}</span><span class="s2">&#39; graph to a checkpoint `</span><span class="si">{}</span><span class="s2">`:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_str</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">log_str</span><span class="p">)</span></div>

<div class="viewcode-block" id="NeuralGraph.restore_from"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.restore_from">[docs]</a>    <span class="k">def</span> <span class="nf">restore_from</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Restores the state of trainable modules in the graph from a checkpoint file.</span>

<span class="sd">        Args:</span>
<span class="sd">            filename (string): Name of the checkpoint to be restored from.</span>
<span class="sd">            module_names: List of modules to be frozen (Optional). If passed, all modules will be restored.</span>
<span class="sd">        Raises:</span>
<span class="sd">            KeyError: If name of the module won&#39;t be recognized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Work on all modules.</span>
        <span class="k">if</span> <span class="n">module_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

        <span class="c1"># Load the checkpoint.</span>
        <span class="n">chkpt</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>

        <span class="n">log_str</span> <span class="o">=</span> <span class="s2">&quot;Loading modules constituting the &#39;</span><span class="si">{}</span><span class="s2">&#39; graph from the `</span><span class="si">{}</span><span class="s2">` checkpoint :</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">chkpt</span><span class="p">[</span><span class="s2">&quot;header&quot;</span><span class="p">][</span><span class="s2">&quot;name&quot;</span><span class="p">],</span> <span class="n">filename</span>
        <span class="p">)</span>

        <span class="n">warning</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Iterate through the modules one by one.</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">module_names</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Get module.</span>
                <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
                    <span class="c1"># Restore module weights</span>
                    <span class="n">set_state_dict</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">chkpt</span><span class="p">[</span><span class="s2">&quot;modules&quot;</span><span class="p">][</span><span class="n">name</span><span class="p">])</span>
                    <span class="n">log_str</span> <span class="o">+=</span> <span class="s2">&quot;  * Module &#39;</span><span class="si">{}</span><span class="s2">&#39; (</span><span class="si">{}</span><span class="s2">) params loaded</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="n">log_str</span> <span class="o">+=</span> <span class="s2">&quot;  ! Module &#39;</span><span class="si">{}</span><span class="s2">&#39; params not found in checkpoint</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                <span class="n">warning</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Log results.</span>
        <span class="k">if</span> <span class="n">warning</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">log_str</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">log_str</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property checks if graph is &quot;complete&quot;, which means that the graph has:</span>
<span class="sd">            * exactly one DataLayer</span>
<span class="sd">            * zero bound input ports</span>

<span class="sd">        Returns:</span>
<span class="sd">            True or false.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We assume the first modules is DL.</span>
        <span class="n">dl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="k">if</span> <span class="n">dl</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">datalayer</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># We assume there is only one DL in the whole graph.</span>
        <span class="n">has_datalayer</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Iterate through the modules one by one.</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="c1"># Get module type.</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">datalayer</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">has_datalayer</span><span class="p">:</span>
                    <span class="c1"># More than one DL is not acceptable.</span>
                    <span class="k">return</span> <span class="kc">False</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">has_datalayer</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Now check the ports.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Else:</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property checks if graph is &quot;trainable&quot;, which means that the graph:</span>
<span class="sd">            * is in eval mode</span>
<span class="sd">            * has at least one Loss Neural Modules</span>

<span class="sd">        Returns:</span>
<span class="sd">            True or false.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check if gradients are collected in forward pass.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">operation_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">evaluation</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Iterate through the modules one by one.</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="c1"># Get module type.</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">loss</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">True</span>

        <span class="c1"># No loss modules - cannot train the graph using back-propagation.</span>
        <span class="k">return</span> <span class="kc">False</span>

<div class="viewcode-block" id="NeuralGraph.to"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.to">[docs]</a>    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_type</span><span class="p">:</span> <span class="n">DeviceType</span><span class="p">,</span> <span class="n">use_dataparallel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">        Moves the all the (trainable) modules belonging to a given graph to indicated device.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Moving module(s) to </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device_type</span><span class="p">))</span>

        <span class="c1"># Check device.</span>
        <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pt_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">GPU</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="n">ConfigurationError</span><span class="p">(</span><span class="s2">&quot;Coudn&#39;t use GPUs as there is no CUDA installed&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Cannot use data parallel.</span>
                <span class="k">if</span> <span class="n">use_dataparallel</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">ConfigurationError</span><span class="p">(</span><span class="s2">&quot;Coudn&#39;t use Data Parallel as there is only one GPU device found&quot;</span><span class="p">)</span>

            <span class="c1"># Using cuda -- all devices.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pt_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>  <span class="c1"># DeviceType.AllGpu : distributed data parallel.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="n">ConfigurationError</span><span class="p">(</span><span class="s2">&quot;Coudn&#39;t use Distributed Data Parallel as there is no CUDA installed&quot;</span><span class="p">)</span>

            <span class="c1"># Each worker will use a single gpu.</span>
            <span class="n">use_dataparallel</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pt_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_dataparallel</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Using Data Parallelization on </span><span class="si">{}</span><span class="s2"> GPUs!&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()))</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Data Parallel is currently under development, so use if it with caution&quot;</span><span class="p">)</span>

        <span class="c1"># Work on all modules.</span>
        <span class="n">module_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

        <span class="c1"># Iterate through the modules one by one.</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">module_names</span><span class="p">:</span>
            <span class="c1"># Get module.</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
                <span class="c1"># Check if we want to use data parallel.</span>
                <span class="k">if</span> <span class="n">use_dataparallel</span><span class="p">:</span>
                    <span class="c1"># Skip if the user explicitly said the module shouldn&#39;t be parallelized.</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;skip_in_data_parallel&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">module</span><span class="o">.</span><span class="n">skip_in_data_parallel</span><span class="p">:</span>
                        <span class="c1"># Parallize model.</span>
                        <span class="n">module</span> <span class="o">=</span> <span class="n">DataParallel</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

                <span class="c1"># Mode to device.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pt_device</span><span class="p">)</span></div>

<div class="viewcode-block" id="NeuralGraph.configure_data_loader"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.configure_data_loader">[docs]</a>    <span class="k">def</span> <span class="nf">configure_data_loader</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">timeout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">worker_init_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">multiprocessing_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method updates the default parameters of the DataLoader used by a given neural graph.</span>
<span class="sd">        For the details on the function/meanings of the arguments, please refer to:</span>
<span class="sd">        https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Override all old values.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader_config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;dataset&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="s2">&quot;shuffle&quot;</span><span class="p">:</span> <span class="n">shuffle</span><span class="p">,</span>
            <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">sampler</span><span class="p">,</span>
            <span class="s2">&quot;batch_sampler&quot;</span><span class="p">:</span> <span class="n">batch_sampler</span><span class="p">,</span>
            <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="n">num_workers</span><span class="p">,</span>
            <span class="s2">&quot;collate_fn&quot;</span><span class="p">:</span> <span class="n">collate_fn</span><span class="p">,</span>
            <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="n">pin_memory</span><span class="p">,</span>
            <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="n">drop_last</span><span class="p">,</span>
            <span class="s2">&quot;timeout&quot;</span><span class="p">:</span> <span class="n">timeout</span><span class="p">,</span>
            <span class="s2">&quot;worker_init_fn&quot;</span><span class="p">:</span> <span class="n">worker_init_fn</span><span class="p">,</span>
            <span class="s2">&quot;multiprocessing_context&quot;</span><span class="p">:</span> <span class="n">multiprocessing_context</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="c1"># If loader is set - reset it.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader</span> <span class="o">=</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="NeuralGraph.get_batch"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.get_batch">[docs]</a>    <span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">yield_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method yields a single batch. Optionally, instantiates the DataLoader object used by a given graph.</span>
<span class="sd">            </span>
<span class="sd">        Args:</span>
<span class="sd">            yield_dict: Flag used to yield a tuple or a dict - depending on the user needs</span>
<span class="sd">            (DEFAULT: False, i.e. yielding NamedTuples)</span>
<span class="sd">        Returns:</span>
<span class="sd">            Depending on the mode: a batch in the form of a singe namedtuple or of a dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If loader is not set - set it.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Check graph.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_complete</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">ConfigurationError</span><span class="p">(</span><span class="s2">&quot;Cannot get a batch as graph is incomplete!&quot;</span><span class="p">)</span>
            <span class="c1"># Get datalayer/dataset.</span>
            <span class="n">dl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="c1"># Make sure that this is DataLayer/dataset instance.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dl</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader_config</span><span class="p">[</span><span class="s2">&quot;dataset&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dl</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader_config</span><span class="p">[</span><span class="s2">&quot;dataset&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dl</span><span class="o">.</span><span class="n">dataset</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_loader_config</span><span class="p">)</span>

        <span class="c1"># Return tuple or dict - depending on the user needs.</span>
        <span class="k">if</span> <span class="n">yield_dict</span><span class="p">:</span>
            <span class="c1"># Fetch a batch - in the form of a dict.</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dl</span><span class="o">.</span><span class="n">output_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Dict with a single key:value pair.</span>
                    <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pt_device</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">batch</span>
                    <span class="k">yield</span> <span class="p">{</span><span class="nb">list</span><span class="p">(</span><span class="n">dl</span><span class="o">.</span><span class="n">output_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">batch</span><span class="p">}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Yield a dictionary.</span>
                    <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">elem</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pt_device</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">elem</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
                    <span class="k">yield</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dl</span><span class="o">.</span><span class="n">output_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">batch</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Create a tuple class - if required.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dl</span><span class="o">.</span><span class="n">output_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Create a named tuple type enabling to access outputs by attributes (e.g. out.x).</span>
                <span class="n">output_class_name</span> <span class="o">=</span> <span class="n">f</span><span class="s1">&#39;</span><span class="si">{dl.__class__.__name__}</span><span class="s1">Output&#39;</span>
                <span class="n">result_type</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="n">typename</span><span class="o">=</span><span class="n">output_class_name</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="n">dl</span><span class="o">.</span><span class="n">output_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

            <span class="c1"># Fetch a batch - in the form of a tuple.</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dl</span><span class="o">.</span><span class="n">output_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Return a single object.</span>
                    <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pt_device</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">batch</span>
                    <span class="k">yield</span> <span class="n">batch</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Return a tuple.</span>
                    <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">elem</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pt_device</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">elem</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
                    <span class="k">yield</span> <span class="n">result_type</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">__forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">connections</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Graph forward method. In the execution it follows the order of the modules defined in steps and passes data</span>
<span class="sd">        between modules following the connectivity stored in NmTensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Execute modules one by one.</span>
        <span class="k">for</span> <span class="n">step_number</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_steps</span><span class="p">)):</span>
            <span class="c1"># If graph is complete - we already fetched data from module 0 (DL)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_complete</span> <span class="ow">and</span> <span class="n">step_number</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># So lets skip it.</span>
                <span class="k">continue</span>

            <span class="c1"># Get the module.</span>
            <span class="n">module_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_steps</span><span class="p">[</span><span class="n">step_number</span><span class="p">]</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span>

            <span class="c1"># Change the module&#39;s operation mode.</span>
            <span class="n">module</span><span class="o">.</span><span class="n">operation_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">operation_mode</span>

            <span class="c1"># Produce list of arguments that will be passed to a given modules.</span>
            <span class="n">module_args</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="c1"># Do it by:</span>
            <span class="c1"># - harvesing input port names of a given module,</span>
            <span class="c1"># - checking if the input was not bound (in the inner graph),</span>
            <span class="c1"># - checking if we have already tensors leading to that input (in outer graph).</span>
            <span class="k">for</span> <span class="n">input_port_name</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">input_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="c1"># Check if this port was bound in the inner graph.</span>
                <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">has_binding</span><span class="p">(</span><span class="n">step_number</span><span class="p">,</span> <span class="n">input_port_name</span><span class="p">)</span>
                <span class="c1"># If so, then we must pass whatever was passed to that port in the list of arguments.</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">module_args</span><span class="p">[</span><span class="n">input_port_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                    <span class="k">continue</span>

                <span class="c1"># Else: find a tensor that should be passed to the given module&#39;s input.</span>
                <span class="n">producer_step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
                <span class="c1"># Search for producer/port that we should use.</span>
                <span class="k">for</span> <span class="n">connection</span> <span class="ow">in</span> <span class="n">connections</span><span class="p">:</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">connection</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">step_number</span> <span class="o">==</span> <span class="n">step_number</span>
                        <span class="ow">and</span> <span class="n">connection</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">module_name</span> <span class="o">==</span> <span class="n">module_name</span>
                        <span class="ow">and</span> <span class="n">connection</span><span class="o">.</span><span class="n">consumer</span><span class="o">.</span><span class="n">port_name</span> <span class="o">==</span> <span class="n">input_port_name</span>
                    <span class="p">):</span>
                        <span class="c1"># Got the connection!</span>
                        <span class="n">producer_step</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="n">producer</span><span class="o">.</span><span class="n">step_number</span>
                        <span class="c1"># producer_name = connection.producer.module_name</span>
                        <span class="n">producer_port_name</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="n">producer</span><span class="o">.</span><span class="n">port_name</span>
                        <span class="k">break</span>
                <span class="c1"># Make sure we found the producer.</span>
                <span class="k">if</span> <span class="n">producer_step</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Check if this port is not optional.</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">module</span><span class="o">.</span><span class="n">input_ports</span><span class="p">[</span><span class="n">input_port_name</span><span class="p">]</span><span class="o">.</span><span class="n">optional</span><span class="p">:</span>
                        <span class="n">err</span> <span class="o">=</span> <span class="s2">&quot;Couldn&#39;t find the producer of the </span><span class="si">{}</span><span class="s2"> input to the module </span><span class="si">{}</span><span class="s2"> called in step </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">input_port_name</span><span class="p">,</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">step_number</span>
                        <span class="p">)</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
                    <span class="c1"># If it is optional and not provided - simply do nothing...</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Add this to the argument</span>
                    <span class="n">module_args</span><span class="p">[</span><span class="n">input_port_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span><span class="p">[</span><span class="n">producer_step</span><span class="p">][</span><span class="n">producer_port_name</span><span class="p">]</span>

            <span class="c1"># Ok, now we have all keyword arguments. We can call() the module.</span>
            <span class="n">module_outputs</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">force_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">module_args</span><span class="p">)</span>

            <span class="c1"># Collect all the produced output tensors and add them to this graph.</span>
            <span class="n">output_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">output_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Handle the case of a single data produced.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span><span class="p">[</span><span class="n">step_number</span><span class="p">][</span><span class="n">output_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">module_outputs</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Compare module_outputs with the output port definitions.</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">module_outputs</span><span class="p">):</span>
                    <span class="n">err</span> <span class="o">=</span> <span class="s2">&quot;Invalid number of outputs produced by the module &quot;</span>
                    <span class="n">err</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> - expected: `</span><span class="si">{}</span><span class="s2">`, received: `</span><span class="si">{}</span><span class="s2">`&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">module_name</span><span class="p">,</span> <span class="n">output_names</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">module_outputs</span><span class="p">))</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>

                <span class="c1"># Add them to the passed data.</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">output_names</span><span class="p">,</span> <span class="n">module_outputs</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span><span class="p">[</span><span class="n">step_number</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="c1"># Ok, now return only the bound outputs.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Return a single object.</span>
            <span class="n">smp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">producer_step_module_port</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span><span class="p">[</span><span class="n">smp</span><span class="o">.</span><span class="n">step_number</span><span class="p">][</span><span class="n">smp</span><span class="o">.</span><span class="n">port_name</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Return a tuple.</span>
            <span class="n">output_class_name</span> <span class="o">=</span> <span class="n">f</span><span class="s1">&#39;</span><span class="si">{self.__class__.__name__}</span><span class="s1">Output&#39;</span>
            <span class="n">result_type</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="n">typename</span><span class="o">=</span><span class="n">output_class_name</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="c1"># Prepare list values.</span>
            <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">smp</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">producer_step_module_port</span>
                <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span><span class="p">[</span><span class="n">smp</span><span class="o">.</span><span class="n">step_number</span><span class="p">][</span><span class="n">smp</span><span class="o">.</span><span class="n">port_name</span><span class="p">])</span>
            <span class="c1"># Return the object</span>
            <span class="k">return</span> <span class="n">result_type</span><span class="p">(</span><span class="o">*</span><span class="n">values</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># (==0)</span>
            <span class="c1"># Generally this should not happen!</span>
            <span class="k">return</span>

<div class="viewcode-block" id="NeuralGraph.forward"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Graph forward method. In the execution it follows the order of the modules defined in steps and passes data</span>
<span class="sd">        between modules following the connectivity stored in NmTensors.</span>

<span class="sd">        Accepts a batch (passed as a tuple in args) OR a list of named arguments (as kwargs).</span>

<span class="sd">        Use-case 1:</span>
<span class="sd">        </span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            # Retrieve batch as a tuple and pass it to forward() as tuple.</span>
<span class="sd">            for batch in training_graph.get_batch():</span>
<span class="sd">                training_graph.forward(batch)</span>

<span class="sd">        Use-case 2:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # Retrieve batch as a tuple and pass it to forward() as list of named arguments.</span>
<span class="sd">            for batch in training_graph.get_batch():</span>
<span class="sd">                training_graph.forward(input1=batch.input1, input2=batch.input2)</span>


<span class="sd">        Use-case 3:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # Retrieve batch as dictionary and pass it to forward() as list of named arguments.</span>
<span class="sd">            for batch in training_graph.get_batch(yield_dict=True):</span>
<span class="sd">                training_graph.forward(**batch)</span>

<span class="sd">        Args:</span>
<span class="sd">            args: A tuple object (a batch) with all required inputs (optional)</span>
<span class="sd">            kwargs: a dictionary containing all required inputs (optional)</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple object containing all graph outputs</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Get list of argument names.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_complete</span><span class="p">:</span>
            <span class="c1"># Use dataset definitions.</span>
            <span class="n">dl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="n">input_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">dl</span><span class="o">.</span><span class="n">output_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="c1"># Work on args or kwargs - depending on input_dict settings.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Operate on args, i.e. a single tuple or a single &quot;object&quot; (1 input port to graph).</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Check if it is a tuple.</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;_fields&quot;</span><span class="p">):</span>
                    <span class="n">inputs</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_asdict</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># There is only one input - use the input_names[0] as key.</span>
                    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">err</span> <span class="o">=</span> <span class="s2">&quot;Invalid argument passed to `graph forward()`&quot;</span>
                <span class="n">err</span> <span class="o">+=</span> <span class="s2">&quot; - expected: a tuple, received: `</span><span class="si">{}</span><span class="s2">`&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Operate on named arguments.</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="c1"># Compare inputs with the desired inputs.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">input_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="n">err</span> <span class="o">=</span> <span class="s2">&quot;Invalid list of arguments passed to the `graph forward()`&quot;</span>
            <span class="n">err</span> <span class="o">+=</span> <span class="s2">&quot; - expected: `</span><span class="si">{}</span><span class="s2">`, received: `</span><span class="si">{}</span><span class="s2">`&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_names</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>

        <span class="c1"># Copy inputs to an adequate (module.output-&gt;value) structure.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">{})</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_complete</span><span class="p">:</span>
            <span class="c1"># Treat all the inputs as &quot;DL outputs&quot;.</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="c1"># Create a list of connenctions: (producer.port -&gt; consumer.port)</span>
        <span class="n">connections</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">tensors</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensors</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">connections</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">connections</span><span class="p">())</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">operation_mode</span> <span class="o">==</span> <span class="n">OperationMode</span><span class="o">.</span><span class="n">evaluation</span><span class="p">:</span>
            <span class="c1"># Perform forward - without collecting of the gradients.</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">connections</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Perform forward - and collect all the gradients.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">connections</span><span class="p">)</span></div>

<div class="viewcode-block" id="NeuralGraph.backward"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">losses</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;Tensor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Backward pass through the graph. Optionally the method accepts list of losses to backpropagate from.</span>
<span class="sd">        If not provided, will collect all output of all loss modules in the graph and backpropagate from them.</span>

<span class="sd">        Args:</span>
<span class="sd">            losses: list of losses to backpropagate from (OPTIONAL, DEFAULT: [])</span>

<span class="sd">        Raises:</span>
<span class="sd">            ConfigurationError if graph is non-trainable.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check if we can run backwards at all.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_trainable</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">ConfigurationError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot run backward propagation as the graph `</span><span class="si">{}</span><span class="s2">`` is non-trainable&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="n">losses_to_backpropagate</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># If losses are passed - use those during backpropagation.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">losses_to_backpropagate</span> <span class="o">=</span> <span class="n">losses</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Else: collect outputs of all Loss NMs.</span>
            <span class="k">for</span> <span class="n">step_number</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_steps</span><span class="p">)):</span>

                <span class="c1"># Get the module.</span>
                <span class="n">module_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_steps</span><span class="p">[</span><span class="n">step_number</span><span class="p">]</span>
                <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span>

                <span class="c1"># Collect module outputs.</span>
                <span class="c1"># Assumption: loss modules return only loss.</span>
                <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">loss</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">output_port_names</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">output_ports</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                        <span class="n">losses_to_backpropagate</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_data</span><span class="p">[</span><span class="n">step_number</span><span class="p">][</span><span class="n">output_port_names</span><span class="p">])</span>

        <span class="c1"># Estimate the total number of backward passes (one from each tensor).</span>
        <span class="n">total_passes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses_to_backpropagate</span><span class="p">)</span>

        <span class="c1"># All but the last call to backward should have the retain_graph=True option.</span>
        <span class="n">pass_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">losses_to_backpropagate</span><span class="p">:</span>
            <span class="n">pass_counter</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">pass_counter</span> <span class="o">==</span> <span class="n">total_passes</span><span class="p">:</span>
                <span class="c1"># Last pass.</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># &quot;Other pass.&quot;</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

<div class="viewcode-block" id="NeuralGraph.parameters"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.parameters">[docs]</a>    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            recurse (bool): if True, then yields parameters of graph modules and all their submodules.</span>
<span class="sd">        Returns:</span>
<span class="sd">            An iterator over parameter of all trainable modules.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
            <span class="c1"># Get module.</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
                <span class="c1"># Yield it parameters.</span>
                <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
                    <span class="k">yield</span> <span class="n">param</span></div>

<div class="viewcode-block" id="NeuralGraph.named_parameters"><a class="viewcode-back" href="../../../api-docs/nemo.html#nemo.core.neural_graph.NeuralGraph.named_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            recurse (bool): if True, then yields parameters of graph modules and all their submodules.</span>
<span class="sd">        Returns:</span>
<span class="sd">            An iterator ovar all named parameters of all trainable components.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
            <span class="c1"># Get module.</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">ModuleType</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
                <span class="c1"># Yield it parameters.</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
                    <span class="k">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2020, NVIDIA

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>